{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Distinction Task 12: MDP and RL Algorithms"
      ],
      "metadata": {
        "id": "V42CdTa5EBtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Tic-Tac-Toe (DIP & OCP) & Minimax Algorithm\n",
        "##### Using following Reference to create Open-Closed Principles (OCP) and Dependency Inversion Principles (DIP)\n",
        "- https://docs.python.org/3/library/abc.html\n",
        "- https://towardsdatascience.com/how-to-use-abstract-classes-in-python-d4d2ddc02e90"
      ],
      "metadata": {
        "id": "_vtaVmEXEUyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependency Inversion Principles (DIP)\n",
        "#####*‘A dependency exists when a change in some element of software may cause changes in another element of software’ [1].*\n",
        "\n",
        "-       Reduce dependencies & make them explicit & identify each.\n",
        "-       The program is dependent on abstract classes not on concrete classes.\n",
        "-       High level modules not dependent on low level modules, additionally details only in low level modules depend on the abstraction.\n",
        "-       Your dependencies point upwards (abstraction)\n"
      ],
      "metadata": {
        "id": "c7y2nPc0Eduy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Open-Closed Principle (OCP)\n",
        "-       Classes open for extension but closed for modifiable.\n",
        "-        ‘hinge points in a design’ [1], OCD with LSP = DIP.\n",
        "-       ‘…we want to be able to change what the modules do, without changing the source code of the modules’ [2].\n",
        "\n",
        "##Liskov Substitution Principle (LSP)\n",
        "* Related classes conforming to each other, if:\n",
        "1. Sub-typing but no code extension (abstract class/ interfaces)\n",
        "2. Code extension but no sub-typing (composition) polymorphism\n",
        "-      Avoiding private variables/ operations in parent/ Super-Type class\n",
        "-      Avoiding implementing new variables/ operations in child class/ Sub-Type\n",
        "-      Over-ridden methods in child/ Sub-Type class accept the same input/ output values as parent/ Super-Type class.\n"
      ],
      "metadata": {
        "id": "5hIEguSDEi2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UML Design"
      ],
      "metadata": {
        "id": "MVrfNj0ZEpTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Board Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "Oo_jlijtEwiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random\n",
        "\n",
        "# Library for module abstract class\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Define the abstract board\n",
        "class AbstractBoard(ABC):\n",
        "    def __init__(self, board_size): # Initialize the object's attributes\n",
        "        self.board_size = board_size # Defines the size\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_board(self, placement, state):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_board_state(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_board_size(self):\n",
        "        return self.board_size\n",
        "\n",
        "    @abstractmethod\n",
        "    def insert_letter(self, letter, position):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_full(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def print_board(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def space_is_free(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset_board(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Xw7JIh4EEzCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Board Definition\n",
        "#### Child/ Sub-Type Class\n",
        "#### Define the method to make it concrete"
      ],
      "metadata": {
        "id": "4AcYiClxFMTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concrete class for board\n",
        "class Board(AbstractBoard):\n",
        "    def __init__(self, board_size):\n",
        "        self.board_size = board_size\n",
        "        self.board_data = {i: ' ' for i in range(1, board_size * board_size + 1)}\n",
        "\n",
        "    def set_board(self, placement, state):\n",
        "        self.board_data[placement] = state\n",
        "\n",
        "    def get_board_state(self):\n",
        "        return self.board_data\n",
        "\n",
        "    def get_board_size(self):\n",
        "        return self.board_size\n",
        "\n",
        "    def insert_letter(self, letter, position):\n",
        "        self.set_board(position, letter)\n",
        "\n",
        "    def is_full(self):\n",
        "        return all(val != ' ' for val in self.board_data.values())\n",
        "\n",
        "    def print_board(self):\n",
        "        for row in range(self.board_size):\n",
        "            for col in range(self.board_size):\n",
        "                position = row * self.board_size + col + 1\n",
        "                print(self.board_data[position], end='')\n",
        "                if col < self.board_size - 1:\n",
        "                    print('|', end='')\n",
        "            print()\n",
        "            if row < self.board_size - 1:\n",
        "                print('-' * (self.board_size * 2 - 1))\n",
        "\n",
        "    def space_is_free(self, position):\n",
        "        return self.board_data[position] == ' '\n",
        "\n",
        "    def reset_board(self):\n",
        "        self.board_data = {i: ' ' for i in range(1, self.board_size * self.board_size + 1)}\n",
        "\n",
        "    def check_board_state(self, letter):\n",
        "            size = self.board_size\n",
        "            for row in range(size): # Check rows\n",
        "                if all(self.board_data[row * size + col + 1] == letter for col in range(size)):\n",
        "                    return letter\n",
        "\n",
        "            for col in range(size): # Check columns\n",
        "                if all(self.board_data[row * size + col + 1] == letter for row in range(size)):\n",
        "                    return letter\n",
        "\n",
        "            if all(self.board_data[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "                return letter\n",
        "\n",
        "            if all(self.board_data[i * size + size - i] == letter for i in range(size)):\n",
        "                return letter\n",
        "\n",
        "            if all(val != ' ' for val in self.board_data.values()): # Check for draw\n",
        "                return 'D'\n",
        "\n",
        "            return None"
      ],
      "metadata": {
        "id": "2ZagvSMNFOgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Game Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "VE0P93VpFdPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractGame():\n",
        "  def __init__(self, board_data):\n",
        "    self.board_data = board_data # Defines the board\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_win(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_draw(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Y24ZiJ8yFfhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Game Definition\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "q7jD3lAFFlMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(AbstractGame):\n",
        "  def __init__(self, board_data):\n",
        "    # Get access to method of parent/ super type class (board) returning a temp object\n",
        "    super().__init__(board_data)\n",
        "\n",
        "  # Check for Win\n",
        "  def chk_for_win(self, letter):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    size = self.board_data.get_board_size()\n",
        "    for row in range(size): # Check rows\n",
        "        if all(board_state[row * size + col + 1] == letter for col in range(size)):\n",
        "            return True\n",
        "\n",
        "    for col in range(size): # Check columns\n",
        "        if all(board_state[row * size + col + 1] == letter for row in range(size)):\n",
        "            return True\n",
        "\n",
        "    if all(board_state[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "        return True\n",
        "\n",
        "    if all(board_state[i * size + size - i] == letter for i in range(size)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  # Check for Draw\n",
        "  def chk_for_draw(self):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    for key, value in board_state.items(): # Calling tuple unpack to access keys/ values\n",
        "        if value == ' ':\n",
        "            return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "82lehoUbFn30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Player Definition\n",
        "#### Parent/ Super-Type Class\n",
        "#### All details (functions) are in lower module (OCP)"
      ],
      "metadata": {
        "id": "gii5CjRXFvN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractPlayer(ABC):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        self.letter = letter # O for human/bot, X for bot\n",
        "        self.algorithm = algorithm # subclass of abstract algorithm\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_move(self, board):\n",
        "        pass"
      ],
      "metadata": {
        "id": "HkMsKuzkFxp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Player Definition (human)\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "dnTx1WVPF3rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        super().__init__(letter, algorithm)\n",
        "\n",
        "    def get_move(self, board):\n",
        "        while True:\n",
        "            try:\n",
        "                position = int(input(f'Enter position for {self.letter}: '))\n",
        "                if 1 <= position <= len(board) and board[position] == ' ':\n",
        "                    return position\n",
        "                else:\n",
        "                    print('Invalid position, please enter a different position.')\n",
        "            except ValueError:\n",
        "                print('Invalid input. Please enter a valid integer.')\n",
        "\n"
      ],
      "metadata": {
        "id": "3OhxARoOF6w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Player Definition (bot)\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "fmQGdptVF97a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BotPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        super().__init__(letter, algorithm)\n",
        "        self.algorithm = algorithm\n",
        "\n",
        "    def get_move(self, board):\n",
        "        print(\"Bot moves\")\n",
        "        placement = self.algorithm.comp_move(board.get_board_state(), self.letter)\n",
        "        board.insert_letter(self.letter, placement)\n"
      ],
      "metadata": {
        "id": "ZQ48fHRiGB2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Algorithm Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "ZqCSVYZEGIGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Algorithm(ABC):\n",
        "  def __init__(self, board_data):\n",
        "    self.board_data = board_data\n",
        "    self.player = 'O'\n",
        "    self.bot = 'X'\n",
        "\n",
        "    @abstractmethod\n",
        "    def comp_move(self, board_data, letter):\n",
        "      pass"
      ],
      "metadata": {
        "id": "VpdVXa5fGKAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code a solution to solve minimimax() algorithm performance using Alpha Beta Prunning\n",
        "\n",
        "## Concrete Algorithm Definition\n",
        "#### Child/ Sub-Type Class\n",
        "### Minimax() Algorithm\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-DzpLXVMREFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tic-tac-toe game minimax algorithm produced in Modules 1-3.\n",
        "- A recursive algorithm what finds the optimal move in game.\n",
        "- Explores all possible moves in the game and assigning a score to each moved based on the predicted outcome.\n",
        "- Assumes that both players are playing optimally.\n",
        "- Class implements algorithm (abstract base class).\n",
        "\n",
        "I extended the minimax algorithm to find the best score by ‘alpha-beta pruning’ instead of memorisation like I did in Module 2 and 3.\n",
        "- Reduces the number of best potential moves (nodes) that need to be explored in the game.\n",
        "- Finds best score bot can achieve, updating both based on scores found.\n",
        "- Keeps track of alpha and beta,\n",
        "  - When a position/ node found whose score is less than alpha (if it is maximising node) or > beta (minimising node) the algorithm can prune the rest of the subtree as values will not impact final score.\n",
        "- Additionally, I added in a function called get_move which sets a time limit of 5 seconds with a max search depth of 3 moves. This was due to the algorithm crashing google collab and using up all the available free RAM. This can be increased however this appeared to be suitable to this problem.\n"
      ],
      "metadata": {
        "id": "r_dwhc0hNQoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "class Minimax(Algorithm):\n",
        "    def __init__(self, board_data):\n",
        "        super().__init__(board_data)\n",
        "        player = 'O'\n",
        "        bot = 'X'\n",
        "\n",
        "    # Find the best move for the board state and letter, recusrively explore the game/ board state\n",
        "    # Consider all moves and outcomes but use depth limit to prevent infinite recursion and alpha-beta pruning to improve speed\n",
        "    def minimax(self, board_state, depth, maximizing_player, letter, alpha, beta, endTime):\n",
        "        if datetime.datetime.now() > endTime:\n",
        "            self.mTimePassed = True\n",
        "            return 0\n",
        "\n",
        "        result = self.board_data.check_board_state(letter)\n",
        "        if result == letter:\n",
        "            return 100 if maximizing_player else -100\n",
        "        elif result == 'D':\n",
        "            return 0\n",
        "\n",
        "        if depth == 0:\n",
        "            return 0\n",
        "\n",
        "        if maximizing_player:\n",
        "            max_eval = -10000000\n",
        "            for key in board_state.keys():\n",
        "                if self.board_data.space_is_free(key):\n",
        "                    self.board_data.insert_letter(letter, key)\n",
        "                    eval = self.minimax(self.board_data.get_board_state(), depth - 1, False, letter, alpha, beta, endTime)\n",
        "                    self.board_data.set_board(key, ' ')\n",
        "                    max_eval = max(max_eval, eval)\n",
        "                    alpha = max(alpha, eval)\n",
        "                    if beta <= alpha:\n",
        "                        break\n",
        "            return max_eval\n",
        "\n",
        "        else:\n",
        "            min_eval = 10000000\n",
        "            for key in board_state.keys():\n",
        "                if self.board_data.space_is_free(key):\n",
        "                    self.board_data.insert_letter(self.opposite_letter(letter), key)\n",
        "                    eval = self.minimax(self.board_data.get_board_state(), depth - 1, True, letter, alpha, beta, endTime)\n",
        "                    self.board_data.set_board(key, ' ')\n",
        "                    min_eval = min(min_eval, eval)\n",
        "                    beta = min(beta, eval)\n",
        "                    if beta <= alpha:\n",
        "                        break\n",
        "            return min_eval\n",
        "\n",
        "    def opposite_letter(self, letter):\n",
        "        if letter == 'X':\n",
        "            return 'O'\n",
        "        else:\n",
        "            return 'X'\n",
        "\n",
        "    # Function calls minimax method for each move and returns highest score\n",
        "    # Uses python datetime module to stop algorithm computation at 5 seconds\n",
        "    def get_move(self, board_state, letter):\n",
        "        self.mTimePassed = False\n",
        "        endTime = datetime.datetime.now() + datetime.timedelta(0, 5) # Set time limit to 5 seconds to avoid crashing collab\n",
        "        best_score = -10000000\n",
        "        best_move = None\n",
        "        # Board data is the current state of the game board to check available moves\n",
        "        for key in board_state.keys():\n",
        "            if self.board_data.space_is_free(key):\n",
        "                self.board_data.insert_letter(letter, key)\n",
        "                score = self.minimax(self.board_data.get_board_state(), 3, False, letter, -10000000, 10000000, endTime) # Set maximum search depth to 3 moves\n",
        "                self.board_data.set_board(key, ' ')\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_move = key\n",
        "                if score == 100:\n",
        "                    break\n",
        "        return best_move\n",
        "\n",
        "    # Calls the get_move function to generate a move for the bot\n",
        "    def comp_move(self, board_state, letter):\n",
        "        return self.get_move(board_state, letter)"
      ],
      "metadata": {
        "id": "3GaaTouaGsFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimax() Algorithm Testing\n",
        "- Text 5x5"
      ],
      "metadata": {
        "id": "JvS9UOOJG5-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board = Board(board_size)\n",
        "        game_play = Game(board)\n",
        "        print_board = board.print_board\n",
        "\n",
        "        algorithm = None # Initialize algorithm\n",
        "\n",
        "        while True: # Loop until algorithm selected\n",
        "          # Player selects algorithm\n",
        "          algorithm_choice = input(\"Select Algorithm for the game ('1' or 'random'): \")\n",
        "\n",
        "          if algorithm_choice == '1':\n",
        "            algorithm = Minimax(board_data = board)\n",
        "            bot = BotPlayer('X', algorithm)\n",
        "            break # Exit menu loop\n",
        "\n",
        "          else:\n",
        "            print(\"Currently only minimax algorithm is installed\")\n",
        "\n",
        "        # Bots first move\n",
        "        bot_move = bot.get_move(board)\n",
        "        board.insert_letter('X', bot_move)\n",
        "\n",
        "        # Print board\n",
        "        board.print_board()\n",
        "\n",
        "        player = HumanPlayer('O', None)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "            player_move = player.get_move(board.get_board_state())  # Get move from the human player\n",
        "            board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_play.chk_for_draw():\n",
        "                break\n",
        "\n",
        "            bot_move = bot.get_move(board)\n",
        "            board.insert_letter('X', bot_move)\n",
        "            board.print_board()\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "        else:\n",
        "            print('Draw!')\n",
        "\n",
        "        # Print final board\n",
        "        board.print_board()\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "main()"
      ],
      "metadata": {
        "id": "yNEnX05FGOcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "080a31e6-8512-4d67-9afa-234ccac73238"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' or 'random'): 1\n",
            "Bot moves\n",
            "X| | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "X|O|X| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 10\n",
            "Bot moves\n",
            "X|O|X|X| \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 20\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 30\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X| | |O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 7\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 8\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 11\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X| | | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 13\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X| \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: \n",
            "Invalid input. Please enter a valid integer.\n",
            "Enter position for O: 15\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X| | | |O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 19\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X|X| |O|O\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 22\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            " |O| | | \n",
            "Enter position for O: 21\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            "O|O|X| | \n",
            "Enter position for O: 23\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 24\n",
            "Bot moves\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            "O|O|X|O|X\n",
            "Draw!\n",
            "X|O|X|X|X\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|X|O|X|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            "O|O|X|O|X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the limitations of minimax() algorithm?\n",
        "- There are branching factors in minimax() which makes the process slower than other algorithms [4]\n",
        "- Searching all possible nodes/ branches hinder performance\n",
        "- There are many decisions to make for this algorithm and it’s not possible to explore the entire problem/ tree.\n",
        "- The algorithm assumes the human (player) is making ‘most optimal moves’ but if they use suboptimal moves the algorithm can take longer to determine the next move.\n",
        "- This algorithm uses quiet a lot of memory, if both players are performing at peak, the complexity of the algorithm is O(bm), b is the branching factor and m is the maximum depth of tree (due to DFS).\n",
        "Note: I enhanced this minimax() algorithm using Alpha-Beta pruning as the larger game tree (5x5) requested by the task sheet required it due to standard minimax() limitations.\n"
      ],
      "metadata": {
        "id": "FrNV6u60Nwon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formulate the problem as an MDP, that means, you will have a graph representing each board as a state.\n",
        "\n",
        "States = all possible board configurations\n",
        "Actions = being the moves that the player can make\n",
        "Rewards = +1 for win, -1 for loss, 0 for draw.\n",
        "Transition probabilities – determined by outcomes of player moves\n",
        "\n",
        "To create a graph for MDP,\n",
        "- Nodes would represent all possible board configuration\n",
        "- Transitions between nodes would show the probabilities between states\n",
        "- Each node would represent actions (moves players can play)\n",
        "- This would get very large fast depending on the board size.\n",
        "\n",
        "**Example** a board 3x3 size the graph would have 9 nodes representing possible board (state) configurations. Each node would have 9 outgoing edges/ moves available. The transition probabilities between nodes would be determined by the outcome of the moves.\n",
        "\n",
        "The best way I can think of doing this is to create a class MDP and methods to create the graph, optimal policy and evaluate to find the policy. The policy could be found with RL to iterate over the graph finding the best action to take at each state, that would determine the expected reward.\n",
        "\n"
      ],
      "metadata": {
        "id": "tnDDFdGCOAqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code a solution to solve an MDP using Monte-Carlo RL algorithm (make sure you explore start).\n",
        "### MDP Algorithm\n"
      ],
      "metadata": {
        "id": "lirrE_TWoSqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This algorithm (Monte Carlo RL) is an off-policy method, it learns from the optimal policy instead of the current policy. <br>\n",
        "\n",
        "####I created MonteCarloRL() class which inherits form the algorithm class, however during my implementation I learnt this algorithm learns from multiple games, not just the single game. And it needs lots of games to learn an effective strategy for the reward. I did implement a test against itself before playing with player to optimise the result.\n",
        "- I used reinforcement learning to learn the optimal move policy for the game. It uses state state-action value function Q and a visit count for each state-action pair (n)\n",
        "- Uses an exploration vs exploitation (epsilon) and a discount factor (gamma) to discount future rewards.\n",
        "\n",
        "**comp_move** method uses an epsilon greedy exploration to select a move, and selects the best move based on the state-action values or selects a random move with probability (epsilon).\n",
        "\n",
        "**updated_Q() method** is used to simulate the main game and returns an episode of tuples (state, action, rewards) pairs. The reward function calculates the reward given the game result. <br>\n",
        "<br>\n",
        "\n",
        "**run_episode()** method tests 1000 episodes to find/ learn the most optimal policy (move) and then selects the best one based on the learned state-action values. <br>\n",
        "<br>\n",
        "\n",
        "**State:** The current state of the game board. For Tic Tac Toe, this is represented as a dictionary of the form {position: symbol}, where position is a tuple of x and y coordinates and symbol is the player's symbol ('X' or 'O') or an empty space (' ').<br>\n",
        "<br>\n",
        "\n",
        "**Action:** A possible move that a player can make. For Tic Tac Toe, this is represented as a tuple of x and y coordinates.<br>\n",
        "<br>\n",
        "\n",
        "**State-action pair:** A combination of a state and an action. This represents a particular situation where a player is in a certain state and makes a certain move.<br>\n",
        "<br>\n",
        "\n",
        "**State-action value function (Q):** A function that assigns a numerical value to each state-action pair. This value represents the expected future reward of taking that action in that state. The goal of reinforcement learning is to learn the optimal state-action value function that maximizes the expected future reward.<br>\n",
        "<br>\n",
        "\n",
        "**Visit count (N):** The number of times a particular state-action pair has been visited.<br>\n",
        "<br>\n",
        "\n",
        "**Episode:** A sequence of state-action pairs and rewards that occur during a simulated game or interaction with the environment.<br>\n",
        "<br>\n",
        "\n",
        "**Reward:** Value that reflects how good or bad a particular state is. In Tic Tac Toe, the reward is usually 1 for a win, -1 for a loss, and 0 for a draw.<br>\n",
        "<br>\n",
        "\n",
        "**Discount factor (gamma):** A factor that determines how much to discount future rewards.\n",
        "1. A value of 1 means the future rewards are not discounted at all\n",
        "2.And a value less than 1 means that future rewards are discounted to some extent."
      ],
      "metadata": {
        "id": "x_5cqrJARDTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloRL(Algorithm):\n",
        "    def __init__(self, epsilon=0.1, gamma=1.0):\n",
        "        super().__init__(board_data=None)\n",
        "        self.epsilon = epsilon # Exploration vs Exploitation\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.Q = {} # State-action value function\n",
        "        self.N = {} # Visit count for each state-action pair\n",
        "\n",
        "    # Method uses an epsilon greedy exploration to select a move\n",
        "    # Selects the best move based on the state-action values or selects a random move with probability (epsilon).\n",
        "    def comp_move(self, board_state, letter):\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' '] # Get all valid moves\n",
        "        if random.uniform(0, 1) < self.epsilon: # Exploration\n",
        "            return random.choice(valid_moves)\n",
        "        else: # Exploitation\n",
        "            values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves] # Get state-action values\n",
        "            max_value = max(values)\n",
        "            if values.count(max_value) > 1:\n",
        "                best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "                return random.choice(best_moves)\n",
        "            else:\n",
        "                return valid_moves[values.index(max_value)]\n",
        "\n",
        "    # Method is used to simulate the main game and returns an episode of tuples (state, action, rewards) pairs.\n",
        "    # The reward function calculates the reward given the game result.\n",
        "    def update_Q(self, episode):\n",
        "        G = 0 # Total return\n",
        "        for i, (state, action, reward) in enumerate(episode[::-1]): # Traverse episode backwards\n",
        "            G = self.gamma * G + reward # Calculate discounted return\n",
        "            N_key = (state, action) # State-action pair for visit count/ dictionary\n",
        "            self.N[N_key] = self.N.get(N_key, 0) + 1 # Increment visit count\n",
        "            Q_key = (state, action) # State-action pair for state-action value dictionary\n",
        "            alpha = 1 / self.N[N_key] # Step size\n",
        "            self.Q[Q_key] = self.Q.get(Q_key, 0) + alpha * (G - self.Q.get(Q_key, 0)) # Update state-action value function\n",
        "\n",
        "    # The run_episode() method tests 1000 episodes to find/ learn the most optimal policy (move) and then selects the best one based on the learned state-action values.\n",
        "    def run_episode(self, board, letter):\n",
        "        episode = []\n",
        "        while True:\n",
        "            state = tuple(board.get_board_state().items())\n",
        "            action = self.comp_move(board.get_board_state(), letter)\n",
        "            reward = self.reward_function(board.check_board_state(letter))\n",
        "            episode.append((state, action, reward))\n",
        "            if board.check_board_state(letter) is not None:\n",
        "                break\n",
        "            board.insert_letter(letter, action)\n",
        "            letter = 'O' if letter == 'X' else 'X' # Switch players\n",
        "        return episode\n",
        "\n",
        "    # Takes in the result, outcome of game\n",
        "    # This allows the algorithm to maximise the rewards/ learn most optimal strategy\n",
        "    def reward_function(self, result):\n",
        "        if result == 'O': # Human wins\n",
        "            return -1\n",
        "        elif result == 'X': # Bot wins\n",
        "            return 1\n",
        "        else:\n",
        "            return 0 # Tie\n",
        "\n",
        "\n",
        "    # Algorithm to find the best move for the current game board state.\n",
        "    # run_episode() method 1000 times, this plays a number of gamest against a copy of itself\n",
        "    # Updates the Q values used to make move decisions. Finds valid moves form the board_state\n",
        "    # Selects the highest state-action value, if there are multiple moves with high values it will choose randomly.\n",
        "    def get_move(self, board_state, letter):\n",
        "        for i in range(1000): # Run 1000 episodes\n",
        "            board = Board(board_size=3) # Initialize empty board\n",
        "            self.run_episode(board, letter)\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' '] # Get all valid moves\n",
        "        values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves] # Get state-action values\n",
        "        max_value = max(values)\n",
        "        if values.count(max_value) > 1:\n",
        "            best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "            return random.choice(best_moves)\n",
        "        else:\n",
        "            return valid_moves[values.index(max_value)]\n",
        "\n",
        "# Main function to test algorithm\n",
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board = Board(board_size)\n",
        "        game_play = Game(board)\n",
        "        print_board = board.print_board\n",
        "\n",
        "        algorithm = None # Initialize algorithm\n",
        "\n",
        "        while True: # Loop until user selects an algorithm\n",
        "            # Player selects algorithm\n",
        "            algorithm_choice = input(\"Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, or 'random' for random bot): \")\n",
        "            if algorithm_choice == '1':\n",
        "                algorithm = Minimax(board_data = board)\n",
        "                break\n",
        "            elif algorithm_choice == '2':\n",
        "                algorithm = MonteCarloRL(epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == 'random':\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid choice. Please select again.\")\n",
        "\n",
        "        # Bots first move\n",
        "        if algorithm_choice != 'random':\n",
        "            bot = BotPlayer('X', algorithm)\n",
        "            bot_move = bot.get_move(board)\n",
        "            board.insert_letter('X', bot_move)\n",
        "\n",
        "        # Print board\n",
        "        board.print_board()\n",
        "\n",
        "        player = HumanPlayer('O', None)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "            player_move = player.get_move(board.get_board_state())  # Get move from the human player\n",
        "            board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_play.chk_for_draw():\n",
        "                break\n",
        "\n",
        "            if algorithm_choice == 'random':\n",
        "                bot_move = random.choice([k for k, v in board.get_board_state().items() if v == ' ']) # Select random move\n",
        "                board.insert_letter('X', bot_move)\n",
        "            else:\n",
        "                bot_move = bot.get_move(board)\n",
        "                board.insert_letter('X', bot_move)\n",
        "\n",
        "            board.print_board()\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        else:\n",
        "            print('Draw!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdeieX27Ycpv",
        "outputId": "f76fac7c-cb94-4e02-d3a1-8df213e54932"
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 3\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, or 'random' for random bot): 2\n",
            "Bot moves\n",
            " | | \n",
            "-----\n",
            " | | \n",
            "-----\n",
            " |X| \n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            "X| |O\n",
            "-----\n",
            " | | \n",
            "-----\n",
            " |X| \n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "X| |O\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |X|X\n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "X|X|O\n",
            "-----\n",
            " |O|O\n",
            "-----\n",
            " |X|X\n",
            "Enter position for O: 4\n",
            "You win!\n",
            "X|X|O\n",
            "-----\n",
            "O|O|O\n",
            "-----\n",
            " |X|X\n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 3\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, or 'random' for random bot): 2\n",
            "Bot moves\n",
            " | |X\n",
            "-----\n",
            " | | \n",
            "-----\n",
            " | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| |X\n",
            "-----\n",
            "X| | \n",
            "-----\n",
            " | | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "O|O|X\n",
            "-----\n",
            "X| | \n",
            "-----\n",
            "X| | \n",
            "Enter position for O: 3\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "O|O|X\n",
            "-----\n",
            "X| |O\n",
            "-----\n",
            "X| |X\n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "O|O|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            "X|X|X\n",
            "Bot wins!\n",
            "O|O|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            "X|X|X\n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, or 'random' for random bot): 2\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            " | |O| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| |O| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "O| |O| |O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 4\n",
            "Bot moves\n",
            "O| |O|O|O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | | |X\n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 2\n",
            "You win!\n",
            "O|O|O|O|O\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | | |X\n",
            "---------\n",
            " | |X| | \n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, or 'random' for random bot): 2\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 11\n",
            "Bot moves\n",
            "O|X| | | \n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            "O| | |X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            "O| | |X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 10\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| | |X|O\n",
            "---------\n",
            "O| | |X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 12\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| | |X|O\n",
            "---------\n",
            "O|O| |X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 13\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| | |X|O\n",
            "---------\n",
            "O|O|O|X| \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            "X| |X| | \n",
            "Enter position for O: 17\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| | |X|O\n",
            "---------\n",
            "O|O|O|X| \n",
            "---------\n",
            "X|O|X| | \n",
            "---------\n",
            "X| |X| |X\n",
            "Enter position for O: 22\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O| |X|X|O\n",
            "---------\n",
            "O|O|O|X| \n",
            "---------\n",
            "X|O|X| | \n",
            "---------\n",
            "X|O|X| |X\n",
            "Enter position for O: 7\n",
            "Bot moves\n",
            "O|X| |X|O\n",
            "---------\n",
            "O|O|X|X|O\n",
            "---------\n",
            "O|O|O|X| \n",
            "---------\n",
            "X|O|X| |X\n",
            "---------\n",
            "X|O|X| |X\n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|O|X|X|O\n",
            "---------\n",
            "O|O|O|X|X\n",
            "---------\n",
            "X|O|X| |X\n",
            "---------\n",
            "X|O|X| |X\n",
            "Enter position for O: 19\n",
            "Bot moves\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|O|X|X|O\n",
            "---------\n",
            "O|O|O|X|X\n",
            "---------\n",
            "X|O|X|O|X\n",
            "---------\n",
            "X|O|X|X|X\n",
            "Draw!\n",
            "O|X|O|X|O\n",
            "---------\n",
            "O|O|X|X|O\n",
            "---------\n",
            "O|O|O|X|X\n",
            "---------\n",
            "X|O|X|O|X\n",
            "---------\n",
            "X|O|X|X|X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Devise and code a solution to tic-tac-toe using temporal difference algorithm – SARSA"
      ],
      "metadata": {
        "id": "zzN8mqfBd1nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The SARSA algorithm is a type of reinforcement learning, it updates the state-acton value function based on the rewards given and runs episodes of the game.\n",
        "- This is an ‘on policy method’ which learnt from the same policy that selects actions.\n",
        "- It uses a temporal difference to update the state-action value at each step, whereas the previous algorithm Monte-Carlo RL updates the state value function only at the end of the episode.\n",
        "- I created new classes for SARSA algorithm and Q-Learning algorithm, they use new versions of comp_move and update_Q methods for each algorithm.\n",
        "- Algorithm calls the update_Q() method to update the state-action function after each bot move. <br>\n",
        "<br>\n",
        "- Algorithm uses values for the step size, exploration/ exploitation, and discount factor. <br>\n",
        "\n",
        "**comp_move()** selects the next move for the bot based on the current state-action value. If the exploration stage chooses a random move or it uses current knowledge to select a move with the highest found state-action value.\n",
        "- Select one of the best moves randomly to introduce stochasticity (random probability distribution). If there are multiple moves like the previous algorithm it will randomly select one. <br>\n",
        "<br>\n",
        "\n",
        "**update_Q()** updates the state- action value based on rewards (moves previously made) and uses the SARSA update rule to update the state-action pair.\n",
        "<br>\n",
        "<br>\n",
        "**run_episode()** runs a single game and uses the bots first move, plays the game until it finishes and keeps track of the state-action pairs matching rewards during the episode. <br>\n",
        "<br>\n",
        "**reward_function()** maps the result of the game same as previous function. <br>\n",
        "<br>\n",
        "**get_move()** is similar to comp_move but is used to get the bots move when playing against another player and not in a training stage. It selects the move with the highest state-action value.\n"
      ],
      "metadata": {
        "id": "BCjmyisKf1r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SARSA Algorithm\n",
        "class Sarsa(Algorithm):\n",
        "    def __init__(self, alpha=0.5, epsilon=0.1, gamma=1.0):\n",
        "        super().__init__(board_data=None)\n",
        "        self.alpha = alpha # Step size\n",
        "        self.epsilon = epsilon # Exploration vs Exploitation trade-off\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.Q = {} # State-action value function\n",
        "\n",
        "\n",
        "    # Selects the next move for a given player using the exploration/exploitation trade-off.\n",
        "    # If a random number is less than the exploration rate (epsilon), it chooses a random move (exploration).\n",
        "    # Else the move with the highest value according to the state-action value function (exploitation).\n",
        "    # If multiple moves have the same, highest value, it selects one of them randomly.\n",
        "    def comp_move(self, board_state, letter):\n",
        "        # Get all valid moves\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' ']\n",
        "        if not valid_moves:\n",
        "            # There are no more valid moves\n",
        "            return None\n",
        "        # Exploration\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(valid_moves)\n",
        "        else:\n",
        "            # Exploitation\n",
        "            values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves]\n",
        "            max_value = max(values)\n",
        "            if values.count(max_value) > 1:\n",
        "                best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "                return random.choice(best_moves)\n",
        "            else:\n",
        "                return valid_moves[values.index(max_value)]\n",
        "\n",
        "    # Updates the state-action value function using temporal difference (TD) learning.\n",
        "    # Finds the difference between the current state-action value and updates value for the current state and action based on reward received.\n",
        "    def update_Q(self, state, action, reward, next_state, next_action):\n",
        "        Q_key = (state, action) # State-action pair for state-action value dictionary\n",
        "        # Get state-action values for next state\n",
        "        next_state_values = [self.Q.get((next_state, a), 0) for a in range(1, 10)]\n",
        "        # Select the highest value\n",
        "        max_next_state_value = max(next_state_values)\n",
        "        # Update the state-action value for current state and action using TD learning and given step size (alpha)\n",
        "        self.Q[Q_key] = self.Q.get(Q_key, 0) + self.alpha * (reward + self.gamma * max_next_state_value - self.Q.get(Q_key, 0)) # Update state-action value function\n",
        "\n",
        "\n",
        "    # Runs (episode) Sequence of (state, action, reward) tuples that capture the experience of the 2nd player\n",
        "    # selects the next aciton using comp_move, enter s loop to update action\n",
        "    # Computes rewards for the next step, using update_Q method\n",
        "    def run_episode(self, board, letter):\n",
        "        state = tuple(board.get_board_state().items())\n",
        "        action = self.comp_move(board.get_board_state(), letter)\n",
        "        episode = []\n",
        "        while True:\n",
        "            # Compute the reward for the current step\n",
        "            reward = self.reward_function(board.check_board_state(letter))\n",
        "            # Choose the next action using the exploration-exploitation trade-off\n",
        "            next_action = self.comp_move(board.get_board_state(), letter)\n",
        "            # Get the next state\n",
        "            next_state = tuple(board.get_board_state().items())\n",
        "            # Store the state, action, and reward for the current step in the episode list\n",
        "            episode.append((state, action, reward))\n",
        "\n",
        "            # Check if the game has ended\n",
        "            if board.check_board_state(letter) is not None:\n",
        "                break\n",
        "            # Update the board with the chosen action\n",
        "            board.insert_letter(letter, action)\n",
        "            letter = 'O' if letter == 'X' else 'X' # Switch players\n",
        "            # Compute the reward for the next step\n",
        "            next_reward = self.reward_function(board.check_board_state(letter))\n",
        "            # Update the state-action value function using TD learning\n",
        "            self.update_Q(state, action, reward, next_state, next_action)\n",
        "            # Set the current state, action, and reward to the next state, action, and reward for the next iteration of the loop\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            reward = next_reward\n",
        "        return episode # Return the episode (state, action, reward) tuples\n",
        "\n",
        "    def reward_function(self, result):\n",
        "        if result == 'O':\n",
        "            return -1\n",
        "        elif result == 'X':\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # Handle the exploration/exploitation trade-off.\n",
        "    def get_move(self, board_state, letter):\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' '] # Get all valid moves\n",
        "        # Get state-action values\n",
        "        values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves]\n",
        "        max_value = max(values)\n",
        "        # Select one of the best moves randomly to introduce stochasticity\n",
        "        if values.count(max_value) > 1:\n",
        "            best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "            return random.choice(best_moves)\n",
        "        # Choose the move with the highest value\n",
        "        else:\n",
        "            return valid_moves[values.index(max_value)]\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board = Board(board_size)\n",
        "        game_play = Game(board)\n",
        "        print_board = board.print_board\n",
        "\n",
        "        algorithm = None # Initialize algorithm variable\n",
        "\n",
        "        while True: # Loop until user selects an algorithm\n",
        "            # Player selects algorithm\n",
        "            algorithm_choice = input(\"Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): \")\n",
        "            if algorithm_choice == '1':\n",
        "                algorithm = Minimax(board_data = board)\n",
        "                break\n",
        "            elif algorithm_choice == '2':\n",
        "                algorithm = MonteCarloRL(epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '3':\n",
        "                algorithm = Sarsa(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == 'random':\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid choice. Please select again.\")\n",
        "\n",
        "        # Bots first move\n",
        "        if algorithm_choice != 'random':\n",
        "            bot = BotPlayer('X', algorithm)\n",
        "            bot_move = bot.get_move(board)\n",
        "            board.insert_letter('X', bot_move)\n",
        "\n",
        "        # Print board\n",
        "        board.print_board()\n",
        "\n",
        "        player = HumanPlayer('O', None)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "            player_move = player.get_move(board.get_board_state())  # Get move from the human player\n",
        "            board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_play.chk_for_draw():\n",
        "                break\n",
        "\n",
        "            if algorithm_choice == 'random':\n",
        "                bot_move = random.choice([k for k, v in board.get_board_state().items() if v == ' ']) # Select random move\n",
        "                board.insert_letter('X', bot_move)\n",
        "            else:\n",
        "                bot_move = bot.get_move(board)\n",
        "                board.insert_letter('X', bot_move)\n",
        "\n",
        "            board.print_board()\n",
        "\n",
        "            # Updates the state-action value function for SARSA based on each bot move\n",
        "            if algorithm_choice == '3':\n",
        "                state = tuple(board.get_board_state().items())\n",
        "                action = bot.algorithm.comp_move(board.get_board_state(), 'X')\n",
        "                reward = bot.algorithm.reward_function(board.check_board_state('X'))\n",
        "                next_state = tuple(board.get_board_state().items())\n",
        "                next_action = bot.algorithm.comp_move(board.get_board_state(), 'O')\n",
        "                bot.algorithm.update_Q(state, action, reward, next_state, next_action)\n",
        "\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        else:\n",
        "            print('Draw!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alQdJok-d6nu",
        "outputId": "84542238-46d4-40ea-83cf-6da9941e4efc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 3\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | \n",
            "-----\n",
            " |X| \n",
            "-----\n",
            " | | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "X|O| \n",
            "-----\n",
            " |X| \n",
            "-----\n",
            " | | \n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            "X|O|O\n",
            "-----\n",
            " |X|X\n",
            "-----\n",
            " | | \n",
            "Enter position for O: 7\n",
            "Bot moves\n",
            "X|O|O\n",
            "-----\n",
            " |X|X\n",
            "-----\n",
            "O| |X\n",
            "Bot wins!\n",
            "X|O|O\n",
            "-----\n",
            " |X|X\n",
            "-----\n",
            "O| |X\n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            " |O| | |X\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            " |O|O| |X\n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 4\n",
            "Bot moves\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 11\n",
            "Bot moves\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            " |X| | |X\n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 12\n",
            "Bot moves\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            "O|O| | | \n",
            "---------\n",
            " |X| | |X\n",
            "---------\n",
            " |X| | | \n",
            "Enter position for O: 13\n",
            "Bot moves\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            "O|O|O| | \n",
            "---------\n",
            " |X| |X|X\n",
            "---------\n",
            " |X| | | \n",
            "Enter position for O: 14\n",
            "Bot moves\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "O|O|O|O| \n",
            "---------\n",
            " |X| |X|X\n",
            "---------\n",
            " |X| | | \n",
            "Enter position for O: 15\n",
            "You win!\n",
            "X|O|O|O|X\n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "O|O|O|O|O\n",
            "---------\n",
            " |X| |X|X\n",
            "---------\n",
            " |X| | | \n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            " |O| | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 8\n",
            "Bot moves\n",
            " |O| | | \n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 3\n",
            "Bot moves\n",
            " |O|O| | \n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 4\n",
            "Bot moves\n",
            " |O|O|O| \n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "X| | |X| \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            " |O|O|O|O\n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "X| | |X|X\n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 1\n",
            "You win!\n",
            "O|O|O|O|O\n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | | |X|X\n",
            "---------\n",
            "X| | |X|X\n",
            "---------\n",
            " | | | |X\n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 7\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "X|O| | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 13\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "X|O| | | \n",
            "---------\n",
            " | |O|X| \n",
            "---------\n",
            " | | | |X\n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 19\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "X|O|X| | \n",
            "---------\n",
            " | |O|X| \n",
            "---------\n",
            " | | |O|X\n",
            "---------\n",
            " | |X| | \n",
            "Enter position for O: 25\n",
            "You win!\n",
            "O| | | | \n",
            "---------\n",
            "X|O|X| | \n",
            "---------\n",
            " | |O|X| \n",
            "---------\n",
            " | | |O|X\n",
            "---------\n",
            " | |X| |O\n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | |X| \n",
            "---------\n",
            "X| | | | \n",
            "Enter position for O: 7\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " |O| | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | | | \n",
            "Enter position for O: 13\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " |O| | |X\n",
            "---------\n",
            " | |O| | \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | | | \n",
            "Enter position for O: 14\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " |O| | |X\n",
            "---------\n",
            " | |O|O| \n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | |X| \n",
            "Enter position for O: 15\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " |O| |X|X\n",
            "---------\n",
            " | |O|O|O\n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | |X| \n",
            "Enter position for O: 12\n",
            "Bot moves\n",
            "O| | | |X\n",
            "---------\n",
            " |O| |X|X\n",
            "---------\n",
            " |O|O|O|O\n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | |X| \n",
            "Enter position for O: 12\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 11\n",
            "You win!\n",
            "O| | | |X\n",
            "---------\n",
            " |O| |X|X\n",
            "---------\n",
            "O|O|O|O|O\n",
            "---------\n",
            " | |X|X| \n",
            "---------\n",
            "X| | |X| \n",
            "Do you want to play again? (y/n): y\n",
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, or 'random' for random bot): 3\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 11\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "O| | | | \n",
            "---------\n",
            "O| |X| | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 12\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            "O|X| | | \n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "O|O|X| | \n",
            "---------\n",
            "O|X| | | \n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 8\n",
            "Bot moves\n",
            "O|O|X| |X\n",
            "---------\n",
            "O|X|O| | \n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " | | | |X\n",
            "Enter position for O: 4\n",
            "Bot moves\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O| | \n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " |X| | |X\n",
            "Enter position for O: 15\n",
            "Bot moves\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O| | \n",
            "---------\n",
            "O|O|X| |O\n",
            "---------\n",
            "X| |X| | \n",
            "---------\n",
            " |X| |X|X\n",
            "Enter position for O: 10\n",
            "Bot moves\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O| |O\n",
            "---------\n",
            "O|O|X| |O\n",
            "---------\n",
            "X| |X|X| \n",
            "---------\n",
            " |X| |X|X\n",
            "Enter position for O: 8\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 9\n",
            "Bot moves\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O|O|O\n",
            "---------\n",
            "O|O|X| |O\n",
            "---------\n",
            "X| |X|X| \n",
            "---------\n",
            "X|X| |X|X\n",
            "Enter position for O: 14\n",
            "Bot moves\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O|O|O\n",
            "---------\n",
            "O|O|X|O|O\n",
            "---------\n",
            "X| |X|X| \n",
            "---------\n",
            "X|X|X|X|X\n",
            "Bot wins!\n",
            "O|O|X|O|X\n",
            "---------\n",
            "O|X|O|O|O\n",
            "---------\n",
            "O|O|X|O|O\n",
            "---------\n",
            "X| |X|X| \n",
            "---------\n",
            "X|X|X|X|X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Devise and code a solution to tic-tac-toe using temporal difference algorithm Q-learning."
      ],
      "metadata": {
        "id": "3gUgqIdsd69Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QLearning algorithm is another reinforcement learning its different to SARSA in how it updates its state-action value.\n",
        "- SARSA uses Temporal Difference TD but QLearning updates by maximum expected future rewards.\n",
        "- It does this for each step where it takes the maximum expected future reward of the next state and all possible actions from the next state, but the next action is not used to update the algorithm.<br>\n",
        "<br>\n",
        "\n",
        "Algorithm starts with default values for the learning rate (alpha), exploration vs exploitation rate (epsilon), and discount factor (gamma). And an empty dictionary to store the state-action value function.<br>\n",
        "<br>\n",
        "**comp_move()** selects the next move for a given player using the exploration/exploitation.\n",
        "- If a random number is less than the exploration rate (epsilon), it chooses a random move (exploration).\n",
        "- Or move with the highest value according to the state-action value/ exploitation. Multiple moves highest value selects randomly.<br>\n",
        "<br>\n",
        "\n",
        "**update_Q()** updates the state-action value function/ computes the next state with the highest state-action value, based on the given discount factor (gamma)\n",
        "- Updates the current state-action value function with the reward received and learning rate (alpha).<br>\n",
        "<br>\n",
        "\n",
        "**run_episode()** runs a sequence of (state, action, reward) tuples track player interaction. Gets current state and selects the next action using comp_move().\n",
        "- Loops to update board based on selected action, rewards for the current/ next steps, updates the state-action value update_Q()\n",
        "- Records the current (state, action, reward). <br>\n",
        "<br>\n",
        "\n",
        "**reward_function()** same as previous methods.<br>\n",
        "<br>\n",
        "**get_move()** selects the next move for board state/ player using state-action value. <br>\n",
        "<br>\n",
        "**comp_move()** and run_episode() to handle the exploration/exploitation. Returns move based on this.\n"
      ],
      "metadata": {
        "id": "TVw-XJKBi-CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uses same comp_move() and run_episode() to handle the exploration/exploitation\n",
        "\n",
        "class QLearning(Algorithm):\n",
        "    def __init__(self, alpha=0.5, epsilon=0.1, gamma=1.0):\n",
        "        super().__init__(board_data=None)\n",
        "        self.alpha = alpha # Step size\n",
        "        self.epsilon = epsilon # Exploration vs Exploitation trade-off\n",
        "        self.gamma = gamma # Discount factor\n",
        "        self.Q = {} # State-action value function\n",
        "\n",
        "    # selects the next move for a given player using the exploration/exploitation\n",
        "    # If a random number is less than the exploration rate (epsilon), it chooses a random move (exploration)\n",
        "    # Or move with the highest value according to the state-action value/ exploitation. Multiple moves highest value selects randomly\n",
        "    def comp_move(self, board_state, letter):\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' '] # Get all valid moves\n",
        "        if random.uniform(0, 1) < self.epsilon: # Exploration\n",
        "            return random.choice(valid_moves)\n",
        "        else: # Exploitation\n",
        "            values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves] # Get state-action values\n",
        "            max_value = max(values)\n",
        "            if values.count(max_value) > 1:\n",
        "                best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "                return random.choice(best_moves)\n",
        "            else:\n",
        "                return valid_moves[values.index(max_value)]\n",
        "\n",
        "    # Updates the state-action value function/ computes the next state with the highest state-action value, based on the given discount factor (gamma)\n",
        "    # Updates the current state-action value function with the reward received and learning rate (alpha)\n",
        "    def update_Q(self, state, action, reward, next_state):\n",
        "        Q_key = (state, action) # State-action pair for state-action value dictionary\n",
        "        next_state_values = [self.Q.get((next_state, a), 0) for a in range(1, 10)]\n",
        "        max_next_state_value = max(next_state_values)\n",
        "        self.Q[Q_key] = self.Q.get(Q_key, 0) + self.alpha * (reward + self.gamma * max_next_state_value - self.Q.get(Q_key, 0)) # Update state-action value function\n",
        "\n",
        "\n",
        "    # Runs a sequence of (state, action, reward) tuples track player interaction\n",
        "    # Gets current state and selects the next action using comp_move()\n",
        "    # Loops tp update board based on selected action, rewards for the current/ next steps\n",
        "    # Updates the state-action value update_Q() and records the current (state, action, reward)\n",
        "    def run_episode(self, board, letter):\n",
        "        state = tuple(board.get_board_state().items())\n",
        "        episode = []\n",
        "        while True:\n",
        "            action = self.comp_move(board.get_board_state(), letter)\n",
        "            reward = self.reward_function(board.check_board_state(letter))\n",
        "            board.insert_letter(letter, action)\n",
        "            next_state = tuple(board.get_board_state().items())\n",
        "            episode.append((state, action, reward))\n",
        "            if board.check_board_state(letter) is not None:\n",
        "                break\n",
        "            letter = 'O' if letter == 'X' else 'X' # Switch players\n",
        "            self.update_Q(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    # Same as previous methods\n",
        "    def reward_function(self, result):\n",
        "        if result == 'O':\n",
        "            return -1\n",
        "        elif result == 'X':\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    # Selects the next move for board state/ player using state-action value\n",
        "    def get_move(self, board_state, letter):\n",
        "        valid_moves = [k for k, v in board_state.items() if v == ' '] # Get all valid moves\n",
        "        values = [self.Q.get((tuple(board_state.items()), move), 0) for move in valid_moves] # Get state-action values\n",
        "        max_value = max(values)\n",
        "        if values.count(max_value) > 1:\n",
        "            best_moves = [valid_moves[i] for i in range(len(valid_moves)) if values[i] == max_value]\n",
        "            return random.choice(best_moves)\n",
        "        else:\n",
        "            return valid_moves[values.index(max_value)]\n",
        "\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board = Board(board_size)\n",
        "        game_play = Game(board)\n",
        "        print_board = board.print_board\n",
        "\n",
        "        algorithm = None # Initialize algorithm\n",
        "\n",
        "        while True: # Loop until user selects an algorithm\n",
        "            # Player selects algorithm\n",
        "            algorithm_choice = input(\"Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, '4' for Q-learning, or 'random' for random bot): \")\n",
        "            if algorithm_choice == '1':\n",
        "                algorithm = Minimax(board_data = board)\n",
        "                break\n",
        "            elif algorithm_choice == '2':\n",
        "                algorithm = MonteCarloRL(epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '3':\n",
        "                algorithm = Sarsa(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '4':\n",
        "                algorithm = QLearning(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == 'random':\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid choice. Please select again.\")\n",
        "\n",
        "        # Bots first move\n",
        "        if algorithm_choice != 'random':\n",
        "            bot = BotPlayer('X', algorithm)\n",
        "            bot_move = bot.get_move(board)\n",
        "            board.insert_letter('X', bot_move)\n",
        "\n",
        "        # Print board\n",
        "        board.print_board()\n",
        "\n",
        "        player = HumanPlayer('O', None)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "            player_move = player.get_move(board.get_board_state())  # Get move from the human player\n",
        "            board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_play.chk_for_draw():\n",
        "                break\n",
        "\n",
        "            if algorithm_choice == 'random':\n",
        "                bot_move = random.choice([k for k, v in board.get_board_state().items() if v == ' ']) # Select random move\n",
        "                board.insert_letter('X', bot_move)\n",
        "            else:\n",
        "                bot_move = bot.get_move(board)\n",
        "                board.insert_letter('X', bot_move)\n",
        "\n",
        "            board.print_board()\n",
        "\n",
        "            if algorithm_choice == '4':\n",
        "                state = tuple(board.get_board_state().items())\n",
        "                action = bot.algorithm.comp_move(board.get_board_state(), 'X')\n",
        "                reward = bot.algorithm.reward_function(board.check_board_state('X'))\n",
        "                next_state = tuple(board.get_board_state().items())\n",
        "                bot.algorithm.update_Q(state, action, reward, next_state)\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        else:\n",
        "            print('Draw!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB14G1tMevR1",
        "outputId": "9e1c1807-74a5-457e-f807-1d823a4a87a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 3\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, '4' for Q-learning, or 'random' for random bot): 4\n",
            "Bot moves\n",
            " | | \n",
            "-----\n",
            " | |X\n",
            "-----\n",
            " | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | \n",
            "-----\n",
            " | |X\n",
            "-----\n",
            "X| | \n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "O|O| \n",
            "-----\n",
            "X| |X\n",
            "-----\n",
            "X| | \n",
            "Enter position for O: 7\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 6\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "O|O| \n",
            "-----\n",
            "X|O|X\n",
            "-----\n",
            "X| |X\n",
            "Enter position for O: 3\n",
            "You win!\n",
            "O|O|O\n",
            "-----\n",
            "X|O|X\n",
            "-----\n",
            "X| |X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the working of your Q-Learning algorithm on Tic-tac-toe on board with size 5x5.\n"
      ],
      "metadata": {
        "id": "igccKvTiEek7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board = Board(board_size)\n",
        "        game_play = Game(board)\n",
        "        print_board = board.print_board\n",
        "\n",
        "        algorithm = None # Initialize algorithm\n",
        "\n",
        "        while True: # Loop until user selects an algorithm\n",
        "            # Player selects algorithm\n",
        "            algorithm_choice = input(\"Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, '4' for Q-learning, or 'random' for random bot): \")\n",
        "            if algorithm_choice == '1':\n",
        "                algorithm = Minimax(board_data = board)\n",
        "                break\n",
        "            elif algorithm_choice == '2':\n",
        "                algorithm = MonteCarloRL(epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '3':\n",
        "                algorithm = Sarsa(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '4':\n",
        "                algorithm = QLearning(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == 'random':\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid choice. Please select again.\")\n",
        "\n",
        "        # Bots first move\n",
        "        if algorithm_choice != 'random':\n",
        "            bot = BotPlayer('X', algorithm)\n",
        "            bot_move = bot.get_move(board)\n",
        "            board.insert_letter('X', bot_move)\n",
        "\n",
        "        # Print board\n",
        "        board.print_board()\n",
        "\n",
        "        player = HumanPlayer('O', None)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "            player_move = player.get_move(board.get_board_state())  # Get move from the human player\n",
        "            board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_play.chk_for_draw():\n",
        "                break\n",
        "\n",
        "            if algorithm_choice == 'random':\n",
        "                bot_move = random.choice([k for k, v in board.get_board_state().items() if v == ' ']) # Select random move\n",
        "                board.insert_letter('X', bot_move)\n",
        "            else:\n",
        "                bot_move = bot.get_move(board)\n",
        "                board.insert_letter('X', bot_move)\n",
        "\n",
        "            board.print_board()\n",
        "\n",
        "            if algorithm_choice == '4':\n",
        "                state = tuple(board.get_board_state().items())\n",
        "                action = bot.algorithm.comp_move(board.get_board_state(), 'X')\n",
        "                reward = bot.algorithm.reward_function(board.check_board_state('X'))\n",
        "                next_state = tuple(board.get_board_state().items())\n",
        "                bot.algorithm.update_Q(state, action, reward, next_state)\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "        else:\n",
        "            print('Draw!')\n",
        "            # Print final board\n",
        "            board.print_board()\n",
        "\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk66Y8NnEfkl",
        "outputId": "43859fca-0fe1-4d2f-daa9-c0724ba4a162"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' for Minimax, '2' for Monte-Carlo RL, '3' for SARSA, '4' for Q-learning, or 'random' for random bot): 4\n",
            "Bot moves\n",
            " | | | | \n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "Enter position for O: 1\n",
            "Bot moves\n",
            "O| | | | \n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            " |X| | | \n",
            "Enter position for O: 5\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | | \n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " |X| | | \n",
            "Enter position for O: 15\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            " | |X| | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            "X| | | | \n",
            "---------\n",
            " |X|X| | \n",
            "Enter position for O: 6\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            "O| |X| | \n",
            "---------\n",
            " | | | |O\n",
            "---------\n",
            "X|X| | | \n",
            "---------\n",
            " |X|X| | \n",
            "Enter position for O: 7\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| | | |O\n",
            "---------\n",
            "X|X| | | \n",
            "---------\n",
            " |X|X| | \n",
            "Enter position for O: 14\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| | |O|O\n",
            "---------\n",
            "X|X|X| | \n",
            "---------\n",
            " |X|X| | \n",
            "Enter position for O: 20\n",
            "Bot moves\n",
            "O| | | |O\n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| | |O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            " |X|X| |X\n",
            "Enter position for O: 2\n",
            "Bot moves\n",
            "O|O|X| |O\n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X| | |O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            " |X|X| |X\n",
            "Enter position for O: 4\n",
            "Bot moves\n",
            "O|O|X|O|O\n",
            "---------\n",
            "O|O|X| | \n",
            "---------\n",
            "X|X| |O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            " |X|X| |X\n",
            "Enter position for O: 5\n",
            "Invalid position, please enter a different position.\n",
            "Enter position for O: 10\n",
            "Bot moves\n",
            "O|O|X|O|O\n",
            "---------\n",
            "O|O|X| |O\n",
            "---------\n",
            "X|X| |O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            "X|X|X| |X\n",
            "Enter position for O: 9\n",
            "Bot moves\n",
            "O|O|X|O|O\n",
            "---------\n",
            "O|O|X|O|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            "X|X|X| |X\n",
            "Bot wins!\n",
            "O|O|X|O|O\n",
            "---------\n",
            "O|O|X|O|O\n",
            "---------\n",
            "X|X|X|O|O\n",
            "---------\n",
            "X|X|X| |O\n",
            "---------\n",
            "X|X|X| |X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the convergence performance of two algorithms\n"
      ],
      "metadata": {
        "id": "5x-f1QBw7UX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zGThPD9VmbRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convergence performance of the two algorithms by running several iterations of the game and recording the number of wins, losses, and draws (status) for each algorithm returning play_game().\n",
        "- The game class checks for each players status the returns a number for calculating the reward.<br>\n",
        "<br>\n",
        "\n",
        "SARSA (on-policy method) and Q-Learning (off-policy method) algorithms are RL algorithms, SARSA only required additional lines of code to update the Q-values based on a new state-action pair.\n",
        "- This testing depends on the size of board and reward used as Q-Learning does appear to be faster (quicker in responses) while SARSA is slightly slower which - I can assume is due to the ‘on policy’ method used.\n",
        "- I implemented both with the same reward function and as the human player makes unreliable moves (even when expected to be optimal it is difficult to compare both). <br>\n",
        "<br>\n",
        "\n",
        "*Convergence refers to the limit of a process and can be a useful analytical tool when evaluating the expected performance of an optimization algorithm* [6].\n",
        "\n",
        "- This means I needed to examine the values of each algorithms process in relation to their behaviour over time.\n",
        "- I could do this by running multiple games and then comparing wins/loses/ draws.\n",
        "- I used matplotlib to plot these, additionally, I found an article on Neptune.ai which explained convergence performance and comparing machine learning algorithms which I found very useful for this task, [6]. <br>\n",
        "<br>\n",
        "\n",
        "**compare_algorithms()** run the number of games, comparing the results of the two algorithms. <br>\n",
        "<br>\n",
        "**compare_algorithm_with_custom_rule()** does the same thing, but I changed the QLearning to use a different Q-value rule with changed alpha rules.\n",
        "- This is based on the bellman equation, it updates the Q value rule, using the calculation of the weighted average of the old Q-value (1-alpha) and the max Q value of the next state (alpha).\n",
        "- The alpha value can be changed for both alogrithms for further testing.\n"
      ],
      "metadata": {
        "id": "HEL_Whr5mbdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plays using algorithms and returns the result and reward of the game.\n",
        "def play_game(algorithm, board_size):\n",
        "    board = Board(board_size)\n",
        "    game_play = Game(board)\n",
        "    bot = BotPlayer('X', algorithm)\n",
        "    while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_play.chk_for_draw():\n",
        "        bot_move = bot.get_move(board)\n",
        "        board.insert_letter('X', bot_move)\n",
        "        if game_play.chk_for_win('X'):\n",
        "            reward = 1  # Reward for winning\n",
        "            break\n",
        "        elif game_play.chk_for_draw():\n",
        "            reward = 0  # Reward for a draw\n",
        "            break\n",
        "        player_move = random.choice([k for k, v in board.get_board_state().items() if v == ' '])\n",
        "        board.insert_letter('O', player_move)\n",
        "        if game_play.chk_for_win('O'):\n",
        "            reward = -1  # Reward for losing\n",
        "            break\n",
        "    else:\n",
        "        # Game ended without a win or draw\n",
        "        reward = -0.1  # Penalty for continuing the game\n",
        "\n",
        "    result = 'Win' if reward == 1 else 'Loss' if reward == -1 else 'Draw'\n",
        "    return result, reward\n",
        "\n",
        "\n",
        "# Checking the win and draw conditions and finding the reward\n",
        "class Game(AbstractGame):\n",
        "  def __init__(self, board_data):\n",
        "    super().__init__(board_data)\n",
        "\n",
        "  # Check for Win\n",
        "  def chk_for_win(self, letter):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    size = self.board_data.get_board_size()\n",
        "    for row in range(size): # Check rows\n",
        "        if all(board_state[row * size + col + 1] == letter for col in range(size)):\n",
        "            return True\n",
        "\n",
        "    for col in range(size): # Check columns\n",
        "        if all(board_state[row * size + col + 1] == letter for row in range(size)):\n",
        "            return True\n",
        "\n",
        "    if all(board_state[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "        return True\n",
        "\n",
        "    if all(board_state[i * size + size - i] == letter for i in range(size)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  # Check for Draw\n",
        "  def chk_for_draw(self):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    for key, value in board_state.items(): # Calling tuple unpack to access keys/ values\n",
        "        if value == ' ':\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "  def reward_function(self, winner):\n",
        "    if winner == 'X':\n",
        "        return 1\n",
        "    elif winner == 'O':\n",
        "        return -1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Additional Q-Learning that uses a specific Q-value update rule\n",
        "class CustomQLearning(QLearning):\n",
        "    def __init__(self, alpha, epsilon, gamma):\n",
        "        super().__init__(alpha, epsilon, gamma)\n",
        "\n",
        "    # Custom Q-value update rule\n",
        "    # Uses weighted average of the old Q-value and the maximum Q-value of the next state\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        # Qvalue rule - use a weighted average update\n",
        "        old_q_value = self.get_q_value(state, action)\n",
        "        max_next_q_value = max(self.q_table[next_state].values())\n",
        "        new_q_value = (1 - self.alpha) * old_q_value + self.alpha * (reward + self.gamma * max_next_q_value)\n",
        "        self.set_q_value(state, action, new_q_value)\n",
        "\n",
        "# Compares the performance of conventional SARSA and Q-Learning algorithms\n",
        "def compare_algorithms(board_size, num_games=20):\n",
        "    print(f'Board Size: {board_size}x{board_size}')\n",
        "    sarsa_results = []\n",
        "    qlearning_results = []\n",
        "\n",
        "    for i in range(num_games):\n",
        "        sarsa = Sarsa(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "        qlearning = QLearning(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "        sarsa_result, sarsa_reward = play_game(sarsa, board_size)\n",
        "        qlearning_result, qlearning_reward = play_game(qlearning, board_size)\n",
        "        sarsa_results.append(sarsa_result)\n",
        "        qlearning_results.append(qlearning_result)\n",
        "        print(f'Game {i + 1}')\n",
        "        print(f'SARSA Result: {sarsa_result}, SARSA Reward: {sarsa_reward}')\n",
        "        print(f'Q-learning Result: {qlearning_result}, Q-learning Reward: {qlearning_reward}')\n",
        "\n",
        "    return sarsa_results, qlearning_results\n",
        "\n",
        "# Compares the performance of a custom variant of Q-Learning and SARSA\n",
        "def compare_algorithm_with_custom_rule(board_size, num_games=20, alpha=0.5):\n",
        "    print(f'Board Size: {board_size}x{board_size}')\n",
        "    sarsa_results = []\n",
        "    custom_qlearning_results = []\n",
        "\n",
        "    for i in range(num_games):\n",
        "        sarsa = Sarsa(alpha=alpha, epsilon=0.1, gamma=1.0)\n",
        "        custom_qlearning = CustomQLearning(alpha=alpha, epsilon=0.1, gamma=1.0)\n",
        "\n",
        "        sarsa_result, sarsa_reward = play_game(sarsa, board_size)\n",
        "        custom_qlearning_result, custom_qlearning_reward = play_game(custom_qlearning, board_size)\n",
        "\n",
        "        sarsa_results.append(sarsa_result)\n",
        "        custom_qlearning_results.append(custom_qlearning_result)\n",
        "\n",
        "        print(f'Game {i + 1}')\n",
        "        print(f'SARSA Result: {sarsa_result}, SARSA Reward: {sarsa_reward}')\n",
        "        print(f'Custom Q-learning Result: {custom_qlearning_result}, Custom Q-learning Reward: {custom_qlearning_reward}')\n",
        "\n",
        "    return sarsa_results, custom_qlearning_results\n",
        "\n",
        "\n",
        "# Plots the results of comparing two algorithms\n",
        "def plot_results(sarsa_results, qlearning_results):\n",
        "    algorithms = ['SARSA', 'Q-learning']\n",
        "    wins = [sarsa_results.count('Win'), qlearning_results.count('Win')]\n",
        "    losses = [sarsa_results.count('Loss'), qlearning_results.count('Loss')]\n",
        "    draws = [sarsa_results.count('Draw'), qlearning_results.count('Draw')]\n",
        "\n",
        "    bar_width = 0.3\n",
        "    r1 = np.arange(len(algorithms))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "\n",
        "    plt.bar(r1, wins, color='green', width=bar_width, label='Wins')\n",
        "    plt.bar(r2, losses, color='red', width=bar_width, label='Losses')\n",
        "    plt.bar(r3, draws, color='blue', width=bar_width, label='Draws')\n",
        "\n",
        "    plt.xlabel('Convergence Performance Algorithms', fontweight='bold')\n",
        "    plt.xticks([r + bar_width for r in range(len(algorithms))], algorithms)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Board size 3x3\n",
        "sarsa_results, qlearning_results = compare_algorithms(3)\n",
        "plot_results(sarsa_results, qlearning_results)\n",
        "\n",
        "# Board size 3x3, Q-value updated rule - different alpha value\n",
        "custom_alpha = 0.3\n",
        "sarsa_results, custom_qlearning_results = compare_algorithm_with_custom_rule(3, alpha=custom_alpha)\n",
        "plot_results(sarsa_results, custom_qlearning_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J7hdPilDAg5A",
        "outputId": "84459305-e907-4d11-c819-5102ee541226"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Board Size: 3x3\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 1\n",
            "SARSA Result: Draw, SARSA Reward: 0\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 2\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Draw, Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 3\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 4\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 5\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 6\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 7\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 8\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Draw, Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 9\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 10\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 11\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 12\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 13\n",
            "SARSA Result: Draw, SARSA Reward: 0\n",
            "Q-learning Result: Draw, Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 14\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 15\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 16\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Win, Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 17\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 18\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 19\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 20\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Q-learning Result: Loss, Q-learning Reward: -1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Board Size: 3x3\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 1\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 2\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 3\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 4\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Draw, Custom Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 5\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 6\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 7\n",
            "SARSA Result: Draw, SARSA Reward: 0\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 8\n",
            "SARSA Result: Draw, SARSA Reward: 0\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 9\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Loss, Custom Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 10\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Draw, Custom Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 11\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Draw, Custom Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 12\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Loss, Custom Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 13\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Loss, Custom Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 14\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 15\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Draw, Custom Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 16\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Loss, Custom Q-learning Reward: -1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 17\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 18\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 19\n",
            "SARSA Result: Win, SARSA Reward: 1\n",
            "Custom Q-learning Result: Draw, Custom Q-learning Reward: 0\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Bot moves\n",
            "Game 20\n",
            "SARSA Result: Loss, SARSA Reward: -1\n",
            "Custom Q-learning Result: Win, Custom Q-learning Reward: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To make your solution efficient, you are expected to code Monte-Carlo Tree Search algorithm and integrate it with Q-Learning algorithm previously coded."
      ],
      "metadata": {
        "id": "qKU0Ms5uD_5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MCTS\n",
        "*MCTS combines the standards of Monte Carlo strategies, which rely upon random sampling and statistical evaluation, with tree-primarily based search techniques *[9].\n",
        "\n",
        "Basically, the Monte Carlo Tree Search Algorithm (MCTS) with Q learning is used to make the bot choose more intelligent moves. <br>\n",
        "<br>\n",
        "- The MCTS is a heuristic based search algorithm used for games when the state space is too large, it constructs a search tree of possible moves, principles of RL [9] and an iteration selecting the highest upper confidence bound (UCB) [10].\n",
        "- This is a heuristic measure and expansion, if the node is unexplored, it will add it to the tree and select a random move to play from that node. It uses simulation to play out the game using random moves until the end of game is reached (reward).\n",
        "- The backpropagation is the nodes information on the path selected from root to outcome of simulation. The MCTS algorithm repeats many times finding the best move.\n",
        "\n",
        "### Q-learning algorithm has issues with large state/ action spaces,\n",
        "- It can’t explore all possible actions for each state and store or these Q-values in a Q table\n",
        "- It doesn’t use a tree structure and can miss opportunities as it can’t explore the state space\n",
        "- It normally only finds one solution and can miss better ones. <br>\n",
        "<br>\n",
        "\n",
        "By merging MCTS with Q-learning to update the value of edges in the Monte Carlo search tree generated by the MCTS algorithm it can update the Q table during each episode.\n",
        "- This means that it can simulate the outcome of many games and use this information to find better solutions rather than a brute force approach which would be to costly (time/ memory). <br>\n",
        "<br>\n",
        "\n",
        "1. I created a Q-learning method that takes a state, an action, a reward, and the next state as inputs/ this updates the Q-values in the dictionary.\n",
        "2. I use the Q-values to choose actions in the simulation instead of randomly selecting them (MCTS).\n",
        "3. Additionally, I created a method for backpropagation (MCTS), I use the Q-values to update the node values instead of a reward. <br>\n",
        "<br>\n",
        "\n",
        "I had to create additional Board, AbstractGame and Game classes as I wanted to test this new algorithm against the other algorithms, I had designed in the task rather than just playing the algorithm like before. The MonteCarloNote is the node class which provides methods for updating node statistics and tree traversal. The MontreCarloTreeSearch is the main algorithm and the MCS_Q_Learning is used for the Monte Carlo algorithm in the MCTS.\n"
      ],
      "metadata": {
        "id": "Z2vdjqHiq8Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "# Concrete class for the board\n",
        "class Board(AbstractBoard):\n",
        "    def __init__(self, board_size, board_data=None):\n",
        "        self.board_size = board_size\n",
        "        if board_data is not None:\n",
        "            self.board_data = board_data\n",
        "        else:\n",
        "            self.board_data = {i: ' ' for i in range(1, board_size * board_size + 1)}\n",
        "\n",
        "    def set_board(self, placement, state):\n",
        "        self.board_data[placement] = state\n",
        "\n",
        "    def get_board_state(self):\n",
        "        return self.board_data\n",
        "\n",
        "    def get_board_size(self):\n",
        "        return self.board_size\n",
        "\n",
        "    def is_full(self):\n",
        "        return all(val != ' ' for val in self.board_data.values())\n",
        "\n",
        "    def print_board(self):\n",
        "        for row in range(self.board_size):\n",
        "            for col in range(self.board_size):\n",
        "                position = row * self.board_size + col + 1\n",
        "                print(self.board_data[position], end='')\n",
        "                if col < self.board_size - 1:\n",
        "                    print('|', end='')\n",
        "            print()\n",
        "            if row < self.board_size - 1:\n",
        "                print('-' * (self.board_size * 2 - 1))\n",
        "\n",
        "    def space_is_free(self, position):\n",
        "        return self.board_data[position] == ' '\n",
        "\n",
        "    def reset_board(self):\n",
        "        self.board_data = {i: ' ' for i in range(1, self.board_size * self.board_size + 1)}\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        return [k for k, v in self.board_data.items() if v == ' ']\n",
        "\n",
        "    def insert_letter(self, letter, position):\n",
        "        self.board_data[position] = letter\n",
        "\n",
        "class AbstractGame():\n",
        "    def __init__(self, board):\n",
        "        self.board = board\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_win(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_draw(self):\n",
        "        pass\n",
        "\n",
        "class Game(AbstractGame):\n",
        "    def __init__(self, board_data):\n",
        "        # Create a new instance of the Board class using the board_data dictionary\n",
        "        self.board = Board(int(math.sqrt(len(board_data))), board_data)\n",
        "        super().__init__(self.board)\n",
        "\n",
        "    # Check for Win\n",
        "    def chk_for_win(self, letter):\n",
        "        board_state = self.board.get_board_state()\n",
        "        size = self.board.get_board_size()\n",
        "        for row in range(size): # Check rows\n",
        "            if all(board_state[row * size + col + 1] == letter for col in range(size)):\n",
        "                return True\n",
        "\n",
        "        for col in range(size): # Check columns\n",
        "            if all(board_state[row * size + col + 1] == letter for row in range(size)):\n",
        "                return True\n",
        "\n",
        "        if all(board_state[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "            return True\n",
        "\n",
        "        if all(board_state[i * size + size - i] == letter for i in range(size)):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Check for Draw\n",
        "    def chk_for_draw(self):\n",
        "        board_state = self.board.get_board_state()\n",
        "        for key, value in board_state.items():\n",
        "            if value == ' ':\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "def game_over(board):\n",
        "    game = Game(board.get_board_state())\n",
        "    return game.chk_for_win('X') or game.chk_for_win('O') or game.chk_for_draw()\n",
        "\n",
        "# Specifically for bot player to determine next move in game\n",
        "# Overrides the get_move() method\n",
        "class MonteCarloPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm, board):\n",
        "        super().__init__(letter, algorithm)\n",
        "        self.board = board\n",
        "\n",
        "    def get_move(self):\n",
        "        placement = self.algorithm.comp_move(self.letter)\n",
        "        self.board.insert_letter(self.letter, placement)\n",
        "\n",
        "\n",
        "class BotPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        super().__init__(letter, algorithm)\n",
        "\n",
        "    def get_move(self, board):\n",
        "        placement = self.algorithm.comp_move(board.get_board_state(), self.letter)\n",
        "        board.insert_letter(self.letter, placement)\n",
        "\n",
        "# Define the Monte Carlo Tree Search algorithm\n",
        "# Algorithm simulates games with random moves from node\n",
        "class MonteCarloNode():\n",
        "    # Each MonteCarloNode instance is responsible for holding a board state, a link to its parent node, and its own list of child nodes.\n",
        "    def __init__(self, current_state, letter=None, parent=None):\n",
        "        self.parent = parent\n",
        "        self.children = {}  # List to hold child nodes\n",
        "\n",
        "        # Track of the number of visits and total reward for this node\n",
        "        self.visits = 0\n",
        "        self.wins = 0\n",
        "        self.untried_actions = current_state.get_valid_moves()\n",
        "        self.current_state = current_state\n",
        "        self.letter = letter\n",
        "\n",
        "    # Add a new child node to the current node with a given action and state\n",
        "    def add_child(self, action):\n",
        "        next_state = self.current_state\n",
        "        next_state.set_board(action, self.letter)\n",
        "        next_letter = 'O' if self.letter == 'X' else 'X'\n",
        "        child = MonteCarloNode(current_state=next_state, letter=next_letter, parent=self)\n",
        "        self.untried_actions.remove(action)\n",
        "        self.children[action] = child\n",
        "        return child\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def is_fully_expanded(self):\n",
        "        return len(self.untried_actions) == 0\n",
        "\n",
        "    def is_terminal_node(self):\n",
        "        return self.current_state.is_full()\n",
        "\n",
        "    # Update the total reward and number of visits for this node based on the reward from the simulation\n",
        "    def update(self, reward):\n",
        "        self.visits += 1\n",
        "        self.wins += reward\n",
        "\n",
        "# Simulates games with random moves from node in tree\n",
        "# Uses tree data structure to store board states and outcomes when visited\n",
        "# Each simulation explores a full game and uses backpropagation of the reward, updating nodes win/ visit count\n",
        "class MonteCarloTreeSearch():\n",
        "    def __init__(self, game, c=1.41):\n",
        "        self.game = game\n",
        "        self.c = c\n",
        "\n",
        "    def selection(self, node):\n",
        "        while not node.is_leaf_node():\n",
        "            ucb_values = [self.ucb(node, child) for child in node.children.values()]\n",
        "            node = node.children.values()[ucb_values.index(max(ucb_values))]\n",
        "        return node\n",
        "\n",
        "    def expansion(self, node):\n",
        "        action = random.choice(node.untried_actions)\n",
        "        next_node = node.add_child(action)\n",
        "        return next_node\n",
        "\n",
        "    def simulation(self, node):\n",
        "        current_state = node.current_state\n",
        "        letter = 'O' if node.letter == 'X' else 'X'\n",
        "        while not game_over(self.game):\n",
        "            possible_moves = current_state.get_valid_moves()\n",
        "            move = random.choice(possible_moves)\n",
        "            current_state.insert_letter(letter, move)\n",
        "            letter = 'O' if letter == 'X' else 'X'\n",
        "        # Reward 1 if the node's letter wins, 0 otherwise\n",
        "        reward = 1 if self.game.chk_for_win(node.letter) else 0\n",
        "        return reward\n",
        "\n",
        "\n",
        "    # Backpropagation of the reward occurs from the leaf node to the root node through the tree updating each node's win and visit count\n",
        "    def backpropagation(self, node, reward):\n",
        "        while node is not None:\n",
        "            node.update(reward)\n",
        "            node = node.parent\n",
        "\n",
        "    # Compute the upper confidence bound (UCB) value for the child node using the given constant c\n",
        "    def ucb(self, parent, child):\n",
        "        if child.visits == 0:\n",
        "            return float('inf')\n",
        "        return child.wins / child.visits + self.c * math.sqrt(math.log(parent.visits) / child.visits)\n",
        "\n",
        "    def run_simulation(self, root):\n",
        "        node = root\n",
        "        while not node.is_terminal_node():\n",
        "            if not node.is_fully_expanded():\n",
        "                # Expand the selected node\n",
        "                node = self.expansion(node)\n",
        "                break\n",
        "            else:\n",
        "                node = self.selection(node)\n",
        "        reward = self.simulation(node)\n",
        "        self.backpropagation(node, reward)\n",
        "\n",
        "    def get_best_move(self, root):\n",
        "        return max(root.children.items(), key=lambda kvp: kvp[1].visits)[0]\n",
        "\n",
        "# Q-Learning-based Monte Carlo Simulation\n",
        "# Implements both algorithms uses Monte Carlo search to find best move and updating Q table\n",
        "class MCS_Q_Learning(Algorithm):\n",
        "    def __init__(self, game, iters=100, c=1.41, alpha=0.5, epsilon=0.1, gamma=1.0):\n",
        "        super().__init__(board_data=None)\n",
        "        self.game = game\n",
        "        self.iters = iters\n",
        "        self.c = c\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = gamma\n",
        "        self.Q = {}\n",
        "\n",
        "    # Uses Monte Carlo search to find best move\n",
        "    def comp_move(self, letter):\n",
        "        root = MonteCarloNode(self.game.board, letter)\n",
        "        for i in range(self.iters):\n",
        "            tree_search = MonteCarloTreeSearch(self.game, self.c)\n",
        "            tree_search.run_simulation(root)\n",
        "        return tree_search.get_best_move(root)\n",
        "\n",
        "    # PLays game and updates results Q table\n",
        "    def run_episode(self, letter):\n",
        "        board = self.game.board\n",
        "        state = tuple(board.get_board_state().items())\n",
        "        episode = []\n",
        "        while True:\n",
        "            action = self.comp_move(letter)\n",
        "            reward = self.game.reward_function(board.check_board_state(letter))\n",
        "            board.insert_letter(letter, action)\n",
        "            next_state = tuple(board.get_board_state().items())\n",
        "            episode.append((state, action, reward))\n",
        "            if board.check_board_state(letter) is not None:\n",
        "                break\n",
        "            letter = 'O' if letter == 'X' else 'X'\n",
        "            QLearning.update_Q(self, state, action, reward, next_state)\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "\n",
        "# Concrete implementation of the game\n",
        "class MonteCarloGame(AbstractGame):\n",
        "    def __init__(self, board_size):\n",
        "        self.board = Board(board_size, {i: ' ' for i in range(1, board_size * board_size + 1)})\n",
        "        super().__init__(self.board)\n",
        "\n",
        "    # Check for Win\n",
        "    def chk_for_win(self, letter):\n",
        "        board_state = self.board.get_board_state()\n",
        "        size = self.board.get_board_size()\n",
        "        for row in range(size): # Check rows\n",
        "            if all(board_state[row * size + col + 1] == letter for col in range(size)):\n",
        "                return True\n",
        "\n",
        "        for col in range(size): # Check columns\n",
        "            if all(board_state[row * size + col + 1] == letter for row in range(size)):\n",
        "                return True\n",
        "\n",
        "        if all(board_state[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "            return True\n",
        "\n",
        "        if all(board_state[i * size + size - i] == letter for i in range(size)):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    # Check for Draw\n",
        "    def chk_for_draw(self):\n",
        "        board_state = self.board.get_board_state()\n",
        "        for key, value in board_state.items():\n",
        "            if value == ' ':\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def get_board_state(self):\n",
        "        return self.board.get_board_state()\n",
        "\n",
        "    def reward_function(self, result):\n",
        "        if result == 'O':\n",
        "            return -1\n",
        "        elif result == 'X':\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "# Main function to test\n",
        "def main():\n",
        "    while True:\n",
        "        # Game Set up\n",
        "        board_size = int(input('Enter board size (3, 5, or 9): '))\n",
        "        board_data = {i: ' ' for i in range(1, board_size * board_size + 1)}\n",
        "        game_play = MonteCarloGame(board_size)\n",
        "        print_board = game_play.board.print_board\n",
        "\n",
        "        algorithm = None  # Initialize algorithm\n",
        "\n",
        "        while True:  # Loop until user selects an algorithm\n",
        "            algorithm_choice = input(\n",
        "                \"Select Algorithm for the game ('1' Minimax, '2' Monte-Carlo RL, '3' SARSA, '4' Q-learning, or 'random' random bot): \")\n",
        "            if algorithm_choice == '1':\n",
        "                algorithm = Minimax(board_data=game_play.board)\n",
        "                break\n",
        "            elif algorithm_choice == '2':\n",
        "                algorithm = MonteCarloRL(epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '3':\n",
        "                algorithm = Sarsa(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == '4':\n",
        "                algorithm = QLearning(alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "                break\n",
        "            elif algorithm_choice == 'random':\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid choice. Please select again.\")\n",
        "\n",
        "        # Set MCS algorithm 1st player\n",
        "        algorithm_MCS = MCS_Q_Learning(game_play, iters=100, c=1.41, alpha=0.5, epsilon=0.1, gamma=1.0)\n",
        "        # Bot's first move\n",
        "        if algorithm_choice != 'random':\n",
        "            bot = MonteCarloPlayer('X', algorithm_MCS, game_play.board)\n",
        "            bot.get_move()\n",
        "\n",
        "        # Print board\n",
        "        game_play.board.print_board()\n",
        "\n",
        "        # 2nd Player\n",
        "        player = BotPlayer('O', algorithm)\n",
        "\n",
        "        while not game_play.chk_for_win('O') and not game_play.chk_for_win('X') and not game_over(game_play):\n",
        "            player_move = player.get_move(game_play.board)  # Get move from 2nd player\n",
        "            game_play.board.insert_letter('O', player_move)\n",
        "\n",
        "            if game_play.chk_for_win('O') or game_over(game_play):\n",
        "                break\n",
        "\n",
        "            if algorithm_choice != 'random':\n",
        "                bot.get_move()\n",
        "\n",
        "            if game_play.chk_for_win('X') or game_over(game_play):\n",
        "                break\n",
        "\n",
        "            game_play.board.print_board()\n",
        "\n",
        "        if game_play.chk_for_win('O'):\n",
        "            print('You win!')\n",
        "            # Print final board\n",
        "            game_play.board.print_board()\n",
        "        elif game_play.chk_for_win('X'):\n",
        "            print('Bot wins!')\n",
        "            # Print final board\n",
        "            game_play.board.print_board()\n",
        "        else:\n",
        "            print('Draw!')\n",
        "            # Print final board\n",
        "            game_play.board.print_board()\n",
        "\n",
        "        play_again = input('Do you want to play again? (y/n): ').lower()\n",
        "        if play_again != 'y':\n",
        "            break\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXYbP1hSEUi4",
        "outputId": "e3ae5782-d0e6-47f7-82fd-05cfa8afe99a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter board size (3, 5, or 9): 5\n",
            "Select Algorithm for the game ('1' Minimax, '2' Monte-Carlo RL, '3' SARSA, '4' Q-learning, or 'random' random bot): 4\n",
            "O|O|O|X|O\n",
            "---------\n",
            "O|X|X|O|X\n",
            "---------\n",
            "X|X|O|X|X\n",
            "---------\n",
            "X|O|O|X|X\n",
            "---------\n",
            "X|O|X|X|X\n",
            "Draw!\n",
            "O|O|O|X|O\n",
            "---------\n",
            "O|X|X|O|X\n",
            "---------\n",
            "X|X|O|X|X\n",
            "---------\n",
            "X|O|O|X|X\n",
            "---------\n",
            "X|O|X|X|X\n",
            "Do you want to play again? (y/n): n\n"
          ]
        }
      ]
    }
  ]
}