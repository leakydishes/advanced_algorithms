{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### HD Task 13: Extend Module 12 Task"
      ],
      "metadata": {
        "id": "V42CdTa5EBtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Tic-Tac-Toe Problem\n",
        "##### Extentions\n",
        "1. Integrate the idea of deep representation learning in your code. For this you can use a deep convolutional neural network to represent the state of the board, and integrate it in your implementation of Q-Learning algorithm.\n",
        "\n",
        "2. Implement a policy gradient algorithm. You can use simple REINFORCE algorithm, do your own research and decide which algorithm you will be implementing.\n",
        "\n",
        "3. Compare the performance of your policy gradient algorithm with deep Q-Learning algorithm."
      ],
      "metadata": {
        "id": "_vtaVmEXEUyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UML Design"
      ],
      "metadata": {
        "id": "MVrfNj0ZEpTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Board Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "Oo_jlijtEwiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import random\n",
        "\n",
        "# Library for module abstract class\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Define the abstract board\n",
        "class AbstractBoard(ABC):\n",
        "    def __init__(self, board_size): # Initialize the object's attributes\n",
        "        self.board_size = board_size # Defines the size\n",
        "\n",
        "    @abstractmethod\n",
        "    def set_board(self, placement, state):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_board_state(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_board_size(self):\n",
        "        return self.board_size\n",
        "\n",
        "    @abstractmethod\n",
        "    def insert_letter(self, letter, position):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_full(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def print_board(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def space_is_free(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset_board(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "Xw7JIh4EEzCH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Board Definition\n",
        "#### Child/ Sub-Type Class\n",
        "#### Define the method to make it concrete"
      ],
      "metadata": {
        "id": "4AcYiClxFMTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Board:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.board = [[' ' for _ in range(size)] for _ in range(size)]\n",
        "        self.turn = 'O'\n",
        "\n",
        "    def print_board(self):\n",
        "        for row in self.board:\n",
        "            print(' | '.join(row))\n",
        "\n",
        "    def get_board_state(self):\n",
        "        board_state = {}\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                board_state[(i, j)] = self.board[i][j]\n",
        "        return board_state\n",
        "\n",
        "    def insert_letter(self, letter, move):\n",
        "        i, j = move\n",
        "        self.board[i][j] = letter\n",
        "\n",
        "    def set_board(self, move, letter):\n",
        "        i, j = move\n",
        "        self.board[i][j] = letter\n",
        "        self.turn = letter\n",
        "\n",
        "    def chk_for_win(self, letter):\n",
        "        for i in range(self.size):\n",
        "            if self.board[i][0] == letter and self.board[i][1] == letter and self.board[i][2] == letter:\n",
        "                return True\n",
        "            if self.board[0][i] == letter and self.board[1][i] == letter and self.board[2][i] == letter:\n",
        "                return True\n",
        "            if self.board[i][0] == letter and self.board[1][1] == letter and self.board[2][2] == letter:\n",
        "                return True\n",
        "            if self.board[0][2] == letter and self.board[1][1] == letter and self.board[2][0] == letter:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def chk_for_draw(self):\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                if self.board[i][j] == ' ':\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "\n",
        "    def get_turn(self):\n",
        "      return self.turn\n"
      ],
      "metadata": {
        "id": "2ZagvSMNFOgD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Game Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "VE0P93VpFdPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractGame():\n",
        "  def __init__(self, board_data):\n",
        "    self.board_data = board_data # Defines the board\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_win(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def chk_for_draw(self):\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "Y24ZiJ8yFfhj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Game Definition\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "q7jD3lAFFlMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(AbstractGame):\n",
        "  def __init__(self, board_data):\n",
        "    # Get access to method of parent/ super type class (board) returning a temp object\n",
        "    #super().__init__(board_data)\n",
        "    self.board_data = board_data\n",
        "\n",
        "  # Check for Win\n",
        "  def chk_for_win(self, letter):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    size = self.board_data.get_board_size()\n",
        "    for row in range(size): # Check rows\n",
        "        if all(board_state[row * size + col + 1] == letter for col in range(size)):\n",
        "            return True\n",
        "\n",
        "    for col in range(size): # Check columns\n",
        "        if all(board_state[row * size + col + 1] == letter for row in range(size)):\n",
        "            return True\n",
        "\n",
        "    if all(board_state[i * size + i + 1] == letter for i in range(size)): # Check diagonals\n",
        "        return True\n",
        "\n",
        "    if all(board_state[i * size + size - i] == letter for i in range(size)):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  # Check for Draw\n",
        "  def chk_for_draw(self):\n",
        "    board_state = self.board_data.get_board_state()\n",
        "    for key, value in board_state.items(): # Calling tuple unpack to access keys/ values\n",
        "        if value == ' ':\n",
        "            return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "82lehoUbFn30"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Player Definition\n",
        "#### Parent/ Super-Type Class\n",
        "#### All details (functions) are in lower module (OCP)"
      ],
      "metadata": {
        "id": "gii5CjRXFvN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractPlayer(ABC):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        self.letter = letter # O for human/bot, X for bot\n",
        "        self.algorithm = algorithm # subclass of abstract algorithm\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_move(self, board):\n",
        "        pass"
      ],
      "metadata": {
        "id": "HkMsKuzkFxp2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Player Definition (human)\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "dnTx1WVPF3rG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        super().__init__(letter, algorithm)\n",
        "\n",
        "    def get_move(self, board):\n",
        "        while True:\n",
        "            try:\n",
        "                position = int(input(f'Enter position for {self.letter}: '))\n",
        "                if 1 <= position <= len(board) and board[position] == ' ':\n",
        "                    return position\n",
        "                else:\n",
        "                    print('Invalid position, please enter a different position.')\n",
        "            except ValueError:\n",
        "                print('Invalid input. Please enter a valid integer.')\n",
        "        return None  # add this line to return None if an invalid position is entered"
      ],
      "metadata": {
        "id": "3OhxARoOF6w-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concrete Player Definition (bot)\n",
        "#### Child/ Sub-Type Class"
      ],
      "metadata": {
        "id": "fmQGdptVF97a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BotPlayer(AbstractPlayer):\n",
        "    def __init__(self, letter, algorithm):\n",
        "        super().__init__(letter, algorithm)\n",
        "\n",
        "    def get_move(self, board):\n",
        "        return i, j  # Return the move as a tuple"
      ],
      "metadata": {
        "id": "ZQ48fHRiGB2W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Abstract Algorithm Definition\n",
        "#### Parent/ Super-Type Class"
      ],
      "metadata": {
        "id": "ZqCSVYZEGIGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Algorithm(ABC):\n",
        "  def __init__(self, board_data):\n",
        "    self.board_data = board_data\n",
        "    self.player = 'O'\n",
        "    self.bot = 'X'\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_move(self, board_data, letter):\n",
        "      pass"
      ],
      "metadata": {
        "id": "VpdVXa5fGKAL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Neural Network (Board State) Q-Learning Algorithm\n",
        "\n",
        "### Use a deep convolutional neural network to represent the state of the board.\n",
        "\n",
        "##### a. Board Representation: CNN Model"
      ],
      "metadata": {
        "id": "G08Yqgb6CeXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In previous modules (SIT320), it was evident that tic-tac-toe (game) could not be solved with ‘classical techniques’ [1, p. 8].\n",
        "\n",
        "#### *For example, Minimax algorithm assumes the player (opponent) will perform moves (actions) in a particular way, this assumption can be invalid/ incorrect.*\n",
        "- To find the most optimal solution DP can use the probabilities of an opponent’s behaviour (each move calculated) or ‘…learn the model of the opponent’s behaviour’ [1, p. 9] for sequential decision problems (tic-tac-toe). <br>\n",
        "- However, many iterations of episodes (games) are required to estimate/ learn probabilities (using a value function method to evaluate all states).\n",
        "\n",
        "\n",
        "#### A type of ANNs is Deep Convolutional Neural Network (CNN), ‘…specialized for processing high-dimensional data arranged in spatial arrays, such as images’ [1, p. 227].\n",
        "- Each layer in CNN creates a multitude of feature maps (pattern of activity within an array of units), each unit performs the same operation on data (within a receptive field).\n",
        "- Using different locations on the arrays of incoming data to store each unit of a feature map, thus units in the same feature map have the same weights.\n",
        "\n",
        "\n",
        "#### Deep Convolutional Neural Network (CNN), a model that can take the board (game) state as input. One or more hidden layers, an output layer (Q value) for all possible moves in the game state.\n",
        "- I used TensorFlow [4] an open-source Machine learning (ML) (Python library from Google) to create the CNN model.\n",
        "- I added padding to the layer before the first Conv2D layer in TensorFlow model when using (3x3 game state size) (MaxPooling2D layer) [4] this is to avoid zero or negative outputs of the model shape, this happens when you pass a model that is too small for the layer parameters.\n",
        "\n",
        "#### I created a CNN model architecture summary, architecture using the TensorFlow, this information in the table is the layers of the model and the output shapes of each layer (number of trainable parameters). This table visualises the CNN model structure. This model has two convolutional layers (input/ output layers) and two middle layers.\n",
        "\n",
        "#### To encode the data for the CNN, need to consider either using a single node and feeding it a unique hash value of the board or using an array with each element encoding the value of the piece (specific position as an integer).<br>\n",
        "- I have prepared the data for the input layer to be, ‘O’ = 1 (9bits), ‘X’ -1 (9bits), and empty space = (0) (9bits), representing a total of 27 bits. The output layer will consist of 9 nodes each representing a position on the board, the values in the output nodes are the Q values for the corresponding moves (the estimate future reward for that given state).\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUMWJw-YGYCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow\n",
        "import tensorflow.compat.v1 as tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T2uyu1pmZ9KO",
        "outputId": "1bd0650f-c485-46fa-edb2-1c699e4d1286"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Installing collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.6.0\n",
            "    Uninstalling keras-2.6.0:\n",
            "      Successfully uninstalled keras-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "Successfully installed keras-2.14.0 ml-dtypes-0.2.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Libraries\n",
        "import numpy as np\n",
        "import numpy as ndarray\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Prepare data: Transform board state to CNN input shape\n",
        "def prepare_input(board_state):\n",
        "    mapping = {'O': 1, 'X': -1, ' ': 0}\n",
        "    # Reshape data for CNN\n",
        "    board = [mapping[letter] for letter in board_state.values()]\n",
        "    # return np.array(board).reshape(5, 5, 1)\n",
        "    return np.array(board).reshape(3, 3, 1)\n",
        "\n",
        "# Represent state of board (CNN board)\n",
        "def build_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(3, 3, 1)), # Input shape\n",
        "        tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1))), # Padding layer\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # Change input shape to (5, 5, 1)\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Test print\n",
        "print(\"Model set up:\")\n",
        "model = build_model()  # Create the model\n",
        "model.summary() # Print model architecture summary\n"
      ],
      "metadata": {
        "id": "l5KfOLH_NVPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d76547-716e-4d03-9b80-d70ad111d239"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model set up:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " zero_padding2d (ZeroPaddin  (None, 5, 5, 1)           0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 3, 3, 32)          320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 1, 1, 64)          18496     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23041 (90.00 KB)\n",
            "Trainable params: 23041 (90.00 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standard librarys\n",
        "import json  # to store learned state\n",
        "import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# settings\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(2)"
      ],
      "metadata": {
        "id": "ffDjcEApRY4M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrate it in your implementation of Q-Learning algorithm.\n",
        "\n",
        "###Floris Laporte’s article Reinforcement from the Ground Up (2020) [2] was used to study Q learning with CNN. I found this topic difficult to implement into the previous tic tac toe game methods, however I still wanted to see how this algorithm effected the game play.\n",
        "\n",
        "[2] https://blog.flaport.net/reinforcement-learning-part-2.html\n"
      ],
      "metadata": {
        "id": "5AMJaVwoQjNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### My aim of this modification is to train the CNN to find playable spaces to predict the best X and O actions (moves) as more layers are added to the network. CNN can use regression to  use less space than tabular format (Q-Learning) and mimic the Q function to predict what the next action (move) might be in the game.\n",
        "\n",
        "#### I found this topic difficult to implement into the previous tic tac toe game methods, however I still wanted to see how this algorithm effected the game play.\n",
        "- The QModel class consists of an embedding layer, three fully connected (linear) layers and ReLU activation functions.\n",
        "- The forward method define how the model processes input data with the save and load methods used for the model parameters. With a epsilon green policy as the strategy used, where the agent will choose random actions with a certain probability (epsilon) to explore moves.\n",
        "- The TicTacToe class, Fig. 3 play method simulates playing different numbers of games and records transitions (state, action, next state and reward). The play_turn method simulates a players turn and checks the outcome, the visualise state is used to show the output to terminal.\n",
        "- The Agent class, Fig. 4 uses the Q learning approach where the agent learns a Q value function to estimate the expected future rewards for each action. The agent takes random actions (probability determined) by the epsilon parameter which can be tested with more time. The best action method returns the best action based on the current Q values and the get action decides this next action (random or based on Q values). The learn method updates these Q values using transitions recorded in each game.\n",
        "\n",
        "### Mean Squared Error (MSE) (learning to mimic another function) is used to implement regression [1], [4]. The output of the CNN is used as the input to the loss function with discounted rewards and maximum Q values (next states) being the updated estimate of the Q function.\n",
        "\n"
      ],
      "metadata": {
        "id": "D7Fl-LM_G01_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set Speed\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(2)\n",
        "\n",
        "class QModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(3, 3)\n",
        "        self.layer1 = torch.nn.Linear(30, 300)\n",
        "        self.layer2 = torch.nn.Linear(300, 300)\n",
        "        self.layer3 = torch.nn.Linear(300, 9)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, states2d, turns):\n",
        "        if not torch.is_tensor(states2d):\n",
        "            states2d = torch.from_numpy(states2d)\n",
        "        if not torch.is_tensor(turns):\n",
        "            turns = torch.from_numpy(turns)\n",
        "        assert states2d.dim() == 3 # batch dimension required\n",
        "        assert turns.dim() == 1 # only dim = batch dim\n",
        "        x = torch.cat([states2d.flatten(1), turns[:,None]], 1)\n",
        "        x = self.relu(self.embedding(x)).flatten(1)\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "    def _serialize_tensor(self, tensor):\n",
        "        if tensor.dim() == 0:\n",
        "            return float(tensor)\n",
        "        return [self._serialize_tensor(t) for t in tensor]\n",
        "\n",
        "    def _deserialize_tensor(self, tensor):\n",
        "        return torch.tensor(tensor, dtype=torch.get_default_dtype())\n",
        "\n",
        "    def save(self, filename):\n",
        "        if not filename.endswith(\".json\"):\n",
        "            filename += \".json\"\n",
        "        with open(filename, \"w\") as file:\n",
        "            json.dump(\n",
        "                {k: self._serialize_tensor(t) for k, t in self.state_dict().items()},\n",
        "                file,\n",
        "            )\n",
        "\n",
        "    def load(self, filename):\n",
        "        if not filename.endswith(\".json\"):\n",
        "            filename += \".json\"\n",
        "        with open(filename, \"r\") as file:\n",
        "            self.load_state_dict(\n",
        "                {k: self._deserialize_tensor(t) for k, t in json.load(file).items()}\n",
        "            )\n",
        "        return self\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self, player1, player2):\n",
        "        self.players = {1: player1, 2: player2} # Players against each other\n",
        "        # Game Outcome (tie, player1 wins, player2 wins)\n",
        "        self._reward = {0: 0, 1: 1, 2: -1}\n",
        "\n",
        "    def play(self, num_games=1, visualize=False):\n",
        "        transitions = []\n",
        "        for _ in range(num_games):\n",
        "            turn = 1\n",
        "            state2d = np.zeros((3,3), dtype=np.int64)\n",
        "            state = (state2d, turn) # full state of the game\n",
        "            for i in range(9):\n",
        "                current_player = self.players[turn]\n",
        "                action = current_player.get_action(state)\n",
        "                next_state, reward = self.play_turn(state, action)\n",
        "                transitions.append(\n",
        "                    (state, action, next_state, reward)\n",
        "                )\n",
        "                if visualize:\n",
        "                    self.visualize_state(next_state, turn)\n",
        "\n",
        "                (state2d, turn) = state = next_state\n",
        "\n",
        "                if turn == 0:\n",
        "                    break\n",
        "        return transitions\n",
        "\n",
        "    # Current current player move check win/ loss\n",
        "    def play_turn(self, state, action):\n",
        "        state2d, turn = state # Find states\n",
        "        next_state2d = state2d.copy()\n",
        "        next_turn = turn % 2 + 1\n",
        "        ax, ay = action // 3, action % 3  # Action two indices\n",
        "\n",
        "        # Check space is legal\n",
        "        if state2d[ax, ay] != 0:  # Check invalid move\n",
        "            next_state2d.fill(0)\n",
        "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
        "            return next_state, self._reward[next_turn]  # next player wins\n",
        "        next_state2d[ax, ay] = turn # apply action\n",
        "\n",
        "        # check if the action resulted in a winner\n",
        "        mask = next_state2d == turn\n",
        "        if (\n",
        "            (mask[0, 0] and mask[1, 1] and mask[2, 2])\n",
        "            or (mask[0, 2] and mask[1, 1] and mask[2, 0])\n",
        "            or (mask[0, 0] and mask[0, 1] and mask[0, 2])\n",
        "            or (mask[1, 0] and mask[1, 1] and mask[1, 2])\n",
        "            or (mask[2, 0] and mask[2, 1] and mask[2, 2])\n",
        "            or (mask[0, 0] and mask[1, 0] and mask[2, 0])\n",
        "            or (mask[0, 1] and mask[1, 1] and mask[2, 1])\n",
        "            or (mask[0, 2] and mask[1, 2] and mask[2, 2])\n",
        "        ):\n",
        "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
        "            return next_state, self._reward[turn]  # current player wins\n",
        "\n",
        "        # if the playing board is full, but no winner found = draw\n",
        "        if (next_state2d != 0).all():  # final draw\n",
        "            next_state = (next_state2d, 0)  # next_turn == 0 -> game over\n",
        "            return next_state, self._reward[0]  # no winner\n",
        "\n",
        "        # if no move winner = next player's turn.\n",
        "        next_state = (next_state2d, next_turn)\n",
        "        return next_state, self._reward[0]  # no winner yet\n",
        "\n",
        "    @staticmethod\n",
        "    # Show Game State\n",
        "    def visualize_state(next_state, turn):\n",
        "        next_state2d, next_turn = next_state\n",
        "        print(f\"player {turn}'s turn:\")\n",
        "        if (next_state2d == 0).all() and turn == 0:\n",
        "            print(\"[invalid state]\\n\\n\")\n",
        "        else:\n",
        "            for i in range(3):\n",
        "                print(\"|\".join([\"O\" if next_state2d[i][j]==1 else \"X\" if next_state2d[i][j]==2 else \" \" for j in range(3)]))\n",
        "                if i < 2:\n",
        "                    print(\"-\"*5)\n",
        "            print(\"\\n\")\n",
        "            # check if the game has ended and if so, who won\n",
        "            mask = next_state2d == turn\n",
        "            win_combinations = [\n",
        "                (mask[0, 0] and mask[1, 1] and mask[2, 2]),\n",
        "                (mask[0, 2] and mask[1, 1] and mask[2, 0]),\n",
        "                (mask[0, 0] and mask[0, 1] and mask[0, 2]),\n",
        "                (mask[1, 0] and mask[1, 1] and mask[1, 2]),\n",
        "                (mask[2, 0] and mask[2, 1] and mask[2, 2]),\n",
        "                (mask[0, 0] and mask[1, 0] and mask[2, 0]),\n",
        "                (mask[0, 1] and mask[1, 1] and mask[2, 1]),\n",
        "                (mask[0, 2] and mask[1, 2] and mask[2, 2]),\n",
        "            ]\n",
        "            if any(win_combinations):\n",
        "                print(f\"player {turn} wins!\\n\")\n",
        "            elif (next_state2d != 0).all():\n",
        "              print(\"Tie!\\n\")"
      ],
      "metadata": {
        "id": "Nqcr9JYI-ovQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent plays by repeating games to find optimal Q Value\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self, qmodel=None, epsilon=0.2, learning_rate=0.01, discount_factor=0.9):\n",
        "\n",
        "        self.qmodel = QModel() if qmodel is None else qmodel\n",
        "        self.learning_rate = learning_rate # Speed Q values get updated\n",
        "        # pytorch Optimizer Update weights of Q Model\n",
        "        self._optimizer = torch.optim.Adam(self.qmodel.parameters(), lr=learning_rate)\n",
        "        self.discount_factor = discount_factor # % Future rewards\n",
        "        self.epsilon = epsilon # Chance of random action\n",
        "\n",
        "    def random_action(self):\n",
        "        return int(np.random.randint(0, 9, 1, dtype=np.int64)) # Find random actions choosen from allowed actions\n",
        "\n",
        "    def best_action(self, state):\n",
        "        with torch.no_grad(): # Best Q values\n",
        "            state2d, turn = state\n",
        "            sign = np.float64(1 - 2 * (turn - 1))\n",
        "            turns = torch.tensor(turn, dtype=torch.int64)[None]  # Reduce Batch\n",
        "            states2d = torch.tensor(state2d, dtype=torch.int64)[None]\n",
        "            qvalues = self.qmodel(states2d, turns)[0]\n",
        "        return np.argmax(sign * qvalues)\n",
        "\n",
        "    # Perform an action\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            # Action random with chance of epsilon = best action\n",
        "            action = self.random_action()\n",
        "        else:\n",
        "            # Q values for current game state\n",
        "            action = self.best_action(state)\n",
        "        return action\n",
        "\n",
        "    # Learn from current action\n",
        "    def learn(self, transitions):\n",
        "      states, actions, next_states, rewards = zip(*transitions)\n",
        "      states2d, turns = zip(*states)\n",
        "      next_states2d, next_turns = zip(*next_states)\n",
        "      turns = torch.tensor(turns, dtype=torch.int64)\n",
        "      next_turns = torch.tensor(next_turns, dtype=torch.int64)\n",
        "      states2d = torch.tensor(states2d, dtype=torch.int64)\n",
        "      next_states2d = torch.tensor(next_states2d, dtype=torch.int64)\n",
        "      actions = torch.tensor(actions, dtype=torch.int64)\n",
        "      rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "      with torch.no_grad():\n",
        "          # Q values for current game state\n",
        "          # Check Game is over or not?\n",
        "          mask = (next_turns > 0).float()\n",
        "          signs = (1 - 2 * (next_turns - 1)).float()\n",
        "          next_qvalues = self.qmodel(next_states2d, next_turns)\n",
        "          expected_qvalues_for_actions = rewards + mask * signs * (\n",
        "              self.discount_factor * torch.max(signs[:, None] * next_qvalues, 1)[0]\n",
        "          )\n",
        "\n",
        "      # update Q values:\n",
        "      qvalues_for_actions = torch.gather(\n",
        "          self.qmodel(states2d, turns), dim=1, index=actions[:, None]\n",
        "      ).view(-1)\n",
        "      loss = torch.nn.functional.smooth_l1_loss(\n",
        "          qvalues_for_actions, expected_qvalues_for_actions\n",
        "      )\n",
        "      self._optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self._optimizer.step()\n",
        "      return loss.item()\n"
      ],
      "metadata": {
        "id": "YwpugRaAQjYV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "np.random.seed(3)\n",
        "torch.manual_seed(1)\n",
        "total_number_of_games = 100000\n",
        "number_of_games_per_batch = 100\n",
        "\n",
        "player = Agent(epsilon=0.7, learning_rate=0.01)\n",
        "game = TicTacToe(player, player)\n",
        "\n",
        "min_loss = np.inf\n",
        "range_ = tqdm.trange(total_number_of_games // number_of_games_per_batch)\n",
        "for i in range_:\n",
        "    transitions = game.play(num_games=number_of_games_per_batch)\n",
        "    np.random.shuffle(transitions)\n",
        "    loss = player.learn(transitions)\n",
        "\n",
        "    if loss < min_loss and loss < 0.01:\n",
        "        min_loss = loss\n",
        "\n",
        "    range_.set_postfix(loss=loss, min_loss=min_loss)\n",
        "\n",
        "player.qmodel.save(\"qmodel.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPfA9KEZFK3R",
        "outputId": "731e4331-2c16-4454-a50a-76543a67bd1b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:57<00:00,  8.54it/s, loss=0.0134, min_loss=0.00929]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "player = Agent(epsilon=0.0)  # epsilon=0 = no random guesses\n",
        "game = TicTacToe(player, player)\n",
        "player.qmodel.load(\"qmodel.json\")\n",
        "\n",
        "# play\n",
        "game.play(num_games=1, visualize=True);\n",
        "print(\"------------------\")"
      ],
      "metadata": {
        "id": "wGWz9K8dIPST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8258cb-7a77-4978-eb04-44426ac97c4b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "player 1's turn:\n",
            " | | \n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " | | \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            " | |X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " | | \n",
            "\n",
            "\n",
            "player 1's turn:\n",
            " | |X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            " |X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O|O\n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 2's turn:\n",
            "O|X|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            "O|O|X\n",
            "\n",
            "\n",
            "Tie!\n",
            "\n",
            "------------------\n",
            "player 1's turn:\n",
            " | | \n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " | | \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            " | |X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " | | \n",
            "\n",
            "\n",
            "player 1's turn:\n",
            " | |X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            " |X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O| \n",
            "\n",
            "\n",
            "player 2's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O| \n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            " |O|O\n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 2's turn:\n",
            "O|X|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            " |O|X\n",
            "\n",
            "\n",
            "player 1's turn:\n",
            "O|X|X\n",
            "-----\n",
            "X|O|O\n",
            "-----\n",
            "O|O|X\n",
            "\n",
            "\n",
            "Tie!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "#### This implementation, combined Q learning with Deep CNN where the agent could learn complex strategies in the game using experience of prior games, this gave the agent more flexibility by allowing it to generalise its moves from one state to another.\n",
        "- I believe these state transitions were efficient in data usage as the training was very quick to compile and the flexible hyperparameters meant I could test different applications (epsilon, discount factor).\n",
        "\n",
        "#### Hyperparameters require evaluation (CNN, Q-Learning) and the training process. Evaluating the performance of AI player against different game strategies to insure effective learning. The benefit of CNN is weight sharing, reducing the number of trainable network parameters (creating a faster algorithm) and avoids overfitting. The classification layer is organised and high reliant on the features extracted and can be used in large-scale networks.\n",
        "\n",
        "- However, I don’t think for larger game state sizes this would be efficient, I struggled to implement a 5x5 board size and with more time I believe I would have proved this hypothesis.\n",
        "- When I tried to change the epochs size it appeared to be very hypersensitive and was time consuming to find the best hyperparameters to test.\n",
        "- Due to time limits I wasn’t able to test the algorithm with other forms of RL, however I found it was able to enhance the RL difficulties into smaller ‘supervised’ [3] tasks where the maximum Q value produced for the Q learning overestimates the value of state actions [1, p. 137].\n"
      ],
      "metadata": {
        "id": "qYVGmvXpHjOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Research Policy Gradient Algorithm\n",
        "#### Policy gradient algorithm.\n",
        "\n",
        "### The main aim of RL agents is to maximise the expected reward when following a policy these are defined using parameters (weights, biases of units in the CNN). Where a network of nodes is to be trained on the weights and bias with feedback on process Gradient Decent [3]. Nodes are arranged in layers input nodes (first layer), middle layers (hidden layers) and output nodes (last layer), deep refers to multiple hidden layers within the CNN, with backpropagation used with middle layers.\n",
        "<br>\n",
        "\n",
        "#### A policy is a distribution that the agent uses to find optimal actions, we can use a deep neural network to increase the probability of finding the most optimal actions which approximates the agent’s policy [10]. From the previous tasks in this unit, I have learnt that policy updates use deep RL algorithms which are either value based, or policy based, this includes DQN discussed in the previous question.\n",
        "<br>\n",
        "\n",
        "#### However, most algorithms use a value function which learns the value of an action to pick the best (optimal) action. A ‘parameterized policy’ [1] selects actions without depending on a value function, it can utilise this to learn a policy parameter.\n",
        "- In RL the aim is to maximise the agent’s performance (finding the most optimal actions) over a duration of time.\n",
        "- Where the probability is the  action a taken at a time t given the environment state s with parameter thus a gradient of performance measured. - This gradient will the proportional to the amount of time spent in each state including the sum of actions pairs and the gradient of that policy (gradient accent).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "#### A CNNs network observes the environment as input and outputs actions selecting according to a SoftMax activation function [11] Fig. 9, alternatively the CNN would be a simple linear regression model only capable of returning ‘0s’. It generates a game (episode) and keeps track of that states, actions, rewards, and the agent’s memory. It than revisits these states at the end of each episode (checking the states, actions, and rewards) to calculate the discounted future returns at each time step. The returns are than used as weights and the agents’ actions as labels to than perform backpropagation to than update the weights of the Deep CNN. The agent repeats for several rotations until the most optimal actions are found.\n",
        "<br>\n",
        "\n",
        "#### This results in a Policy Gradient algorithm REINFORCE, where we look at the overall performance in the agent’s behaviour to guide the policy improvement in terms of cumulative rewards. The RL agent uses the environment starting state to goal state (Monte Carlo Policy Gradient algorithm) [12], Fig 10 and unlike the TL or DP ‘bootstrap’ methods [1, p. 119].\n",
        "\n",
        "- To optimise the policy methods such as Maximisation Likelihood Estimate (MLE) can be used to iteratively adjust the policies parameters to select actions that will lead to higher rewards.\n",
        "- As such policy gradient methods are a type or RL that optimise parameter policy that focus on the agent’s behaviour not just the immediate rewards.\n",
        "\n",
        "<br>\n",
        "### Policy gradient algorithms such as Monte Carlo, Fig 11 have significant advantages, they can learn to take actions for specific probabilities and efficient exploration by approaching deterministic policies ‘asymptotically’ [1, p. 337]. Additionally continuous action spaces are easily handled by policy gradient algorithms where action value methods can lack. <br>\n",
        "<br>\n",
        "### An algorithms performance can also be measured using the policy gradient theorem which does not involve state distribution (policy planning for future actions). And by ‘adding a state-value function as a baseline reduce REINFORCE’s variance without introducing bias’ [1, p. 337]. This reduces bootstrapping methods which introduce bias (TD, DP), however these action value methods can reduce variance.\n"
      ],
      "metadata": {
        "id": "FpPYTKepDCBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the Convergence Performance of Algorithms\n",
        "#### Compare the performance of your policy gradient algorithm with deep Q-Learning algorithm."
      ],
      "metadata": {
        "id": "5x-f1QBw7UX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convergence performance of the two algorithms by running several iterations of the game and recording the number of wins, losses, and draws (status) for each algorithm returning play_game().\n",
        "\n",
        "*Convergence refers to the limit of a process and can be a useful analytical tool when evaluating the expected performance of an optimization algorithm* [6].\n",
        "- To examine the values of each algorithms process in relation to their behaviour over time.\n",
        "- Run multiple games and then comparing wins/loses/ draws.<br>\n",
        "<br>\n",
        "\n",
        "### RL changes the trajectory results in for a multitude of rewards thus Monte Carlo Policy Gradient has ‘high variance but zero bias’ [14]. Whereas TD and DP (DQN) algorithms when used as a step (one action is used with a small change) results in a low variance. This can affect ‘model convergence’ as Policy Gradients are weak to variance (and mass producing samples can impact efficiency), especially in on policy methods where behaviour policy and target policy are the same. Whereas off policy methods can improve exploration/ target policy without creating mass amounts of new samples, the more we know about a model’s environment (dynamic) the less train and error we need to experiment with to find the most optimal policy, Fig. 12.\n",
        "\n",
        "- Many RL methods make assumptions (continuity) assuming that the state space or the control space is continuous (DQN) and Q learning with Deep CNN when in a continuous control space has to many complex steps [14].\n",
        "- This is due to the searching required of the entire control space to find the maximum Q value for the next action, this is computationally very difficult. - Whereas policy gradient algorithms can support continuous control, as it‘optimises the policy directly’ [14] by implementing constraints using policy parameters within the objective function.\n",
        "- However, the choice isn’t necessary one or the other, we can have the best of both outputs by adding value learning to a policy gradient or adding a policy gradient to a RL [14].\n"
      ],
      "metadata": {
        "id": "HEL_Whr5mbdv"
      }
    }
  ]
}